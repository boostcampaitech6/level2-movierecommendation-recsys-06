{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQG9hL8-UQUP"
      },
      "source": [
        "# Multi-VAE\n",
        "\n",
        "이번 미션에서는 [Variational Autoencoders for Collaborative Filtering](https://arxiv.org/abs/1802.05814)에서 제안된 Multi-VAE 기반의 협업 필터링을 구현해보도록 하겠습니다. 다양한 Auto-Encoder 기반의 협업필터링이 제안된 이후에, 가장 강력하다고 평가받는 VAE 기반의 협업 필터링을 이해하는 시간을 갖도록 하겠습니다.\n",
        "\n",
        "- 이 미션은 다음 [코드](https://github.com/younggyoseo/vae-cf-pytorch)를 기반으로 작성되었습니다. 바로 코드를 확인해보지 마시고, 최대한 직접 작성을 해보세요!\n",
        "- 이 미션에서 중요한 부분은 모델 부분입니다. 데이터 전처리 부분은 가볍게 훑어 보시고, 모델 부분을 집중해주세요!\n",
        "\n",
        "### 변경사항\n",
        "1. label을 확률값$\\frac{1}{\\sqrt{n}}$로 조정한 soft label 적용\n",
        "    - 이 과정에서 전처리 함수 sparse2tensor로 조정 (이게 왜 다항분포??)\n",
        "    - 각자 독립인 logistic loss도 실험 예정 -> 다항분포 해결은 이걸로 해결되는건가 싶기도 함\n",
        "    - normalize 덜 smooth하게도 고려 중\n",
        "\n",
        "\n",
        "2. tr_uid: userplays.index -> userplays.user 로 조정 (전체 데이터 셋에 등장하지도 않는 유저번호0번 같은 유령유저가 데이터셋에 포함됨)\n",
        "    - 데이터셋에 모든 interaction 0인 유저들 넘쳐났었음\n",
        "    - normalize 시 divide by 0 해결\n",
        "\n",
        "3. 내일 아침 할거\n",
        "    - userid 질문\n",
        "    - 기본 inference 구현\n",
        "    - public 제출"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7CfnRw7U59C"
      },
      "source": [
        "## 1. 초기 세팅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylGzdjCI2ujg",
        "outputId": "09aaad72-83a5-4420-d27a-c5910250328e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bottleneck in /opt/ml/miniconda3/envs/movie/lib/python3.10/site-packages (1.3.5)\n",
            "Requirement already satisfied: numpy in /opt/ml/miniconda3/envs/movie/lib/python3.10/site-packages (from bottleneck) (1.26.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# bottleneck은 C로 작성된 빠른 NumPy 배열 함수 모음입니다.\n",
        "!pip install bottleneck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bQj6k1mSbxaz"
      },
      "outputs": [],
      "source": [
        "\n",
        "import argparse\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = pd.read_csv('../../data/train/pro_sg/validation_te.csv')\n",
        "b = pd.read_csv('../../data/train/pro_sg/validation_tr.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(98001, 397924)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a.shape[0], b.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uid</th>\n",
              "      <th>sid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>26554</td>\n",
              "      <td>3025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>26554</td>\n",
              "      <td>1681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>26554</td>\n",
              "      <td>201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>26554</td>\n",
              "      <td>3190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>26554</td>\n",
              "      <td>3301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97996</th>\n",
              "      <td>26934</td>\n",
              "      <td>228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97997</th>\n",
              "      <td>26934</td>\n",
              "      <td>1126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97998</th>\n",
              "      <td>26934</td>\n",
              "      <td>235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97999</th>\n",
              "      <td>26934</td>\n",
              "      <td>209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98000</th>\n",
              "      <td>26934</td>\n",
              "      <td>1792</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>98001 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         uid   sid\n",
              "0      26554  3025\n",
              "1      26554  1681\n",
              "2      26554   201\n",
              "3      26554  3190\n",
              "4      26554  3301\n",
              "...      ...   ...\n",
              "97996  26934   228\n",
              "97997  26934  1126\n",
              "97998  26934   235\n",
              "97999  26934   209\n",
              "98000  26934  1792\n",
              "\n",
              "[98001 rows x 2 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uid</th>\n",
              "      <th>sid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>26554</td>\n",
              "      <td>440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>26554</td>\n",
              "      <td>741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>26554</td>\n",
              "      <td>1407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>26554</td>\n",
              "      <td>193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>26554</td>\n",
              "      <td>1041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397919</th>\n",
              "      <td>26934</td>\n",
              "      <td>760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397920</th>\n",
              "      <td>26934</td>\n",
              "      <td>697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397921</th>\n",
              "      <td>26934</td>\n",
              "      <td>3245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397922</th>\n",
              "      <td>26934</td>\n",
              "      <td>1369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397923</th>\n",
              "      <td>26934</td>\n",
              "      <td>3691</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>397924 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          uid   sid\n",
              "0       26554   440\n",
              "1       26554   741\n",
              "2       26554  1407\n",
              "3       26554   193\n",
              "4       26554  1041\n",
              "...       ...   ...\n",
              "397919  26934   760\n",
              "397920  26934   697\n",
              "397921  26934  3245\n",
              "397922  26934  1369\n",
              "397923  26934  3691\n",
              "\n",
              "[397924 rows x 2 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMWYLPFIjon8"
      },
      "source": [
        "## 데이터 다운로드\n",
        "이곳에 대회 사이트(AI Stages)에 있는 data의 URL을 입력해주세요.\n",
        "- 데이터 URL은 변경될 수 있습니다.\n",
        "- 예) `!wget https://aistages-prod-server-public.s3.amazonaws.com/app/Competitions/000176/data/data.tar.gz`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = '../../data/train'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ3W0udmbxa3",
        "outputId": "e06b80ea-6ed1-4406-e03c-e9bfe67f9946"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## 각종 파라미터 세팅\n",
        "parser = argparse.ArgumentParser(description='PyTorch Variational Autoencoders for Collaborative Filtering')\n",
        "\n",
        "\n",
        "parser.add_argument('--data', type=str, default=data_dir,\n",
        "                    help='Movielens dataset location')\n",
        "\n",
        "parser.add_argument('--lr', type=float, default=1e-3,\n",
        "                    help='initial learning rate')\n",
        "parser.add_argument('--wd', type=float, default=0.00,\n",
        "                    help='weight decay coefficient')\n",
        "parser.add_argument('--batch_size', type=int, default=500,\n",
        "                    help='batch size')\n",
        "parser.add_argument('--epochs', type=int, default=20,\n",
        "                    help='upper epoch limit')\n",
        "parser.add_argument('--total_anneal_steps', type=int, default=200000,\n",
        "                    help='the total number of gradient updates for annealing')\n",
        "parser.add_argument('--anneal_cap', type=float, default=0.2,\n",
        "                    help='largest annealing parameter')\n",
        "parser.add_argument('--seed', type=int, default=1111,\n",
        "                    help='random seed')\n",
        "parser.add_argument('--cuda', action='store_true',\n",
        "                    help='use CUDA')\n",
        "parser.add_argument('--log_interval', type=int, default=100, metavar='N',\n",
        "                    help='report interval')\n",
        "parser.add_argument('--save', type=str, default='model.pt',\n",
        "                    help='path to save the final model')\n",
        "args = parser.parse_args([])\n",
        "\n",
        "# Set the random seed manually for reproductibility.\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "#만약 GPU가 사용가능한 환경이라면 GPU를 사용\n",
        "if torch.cuda.is_available():\n",
        "    args.cuda = True\n",
        "\n",
        "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o1fvXqFWE_G"
      },
      "source": [
        "##2. 데이터 전처리\n",
        "\n",
        "이 부분에서 진행되는 과정은 저희가 일반적으로 알고있는 MovieLens (user, item, timestamp)데이터를 전처리하는 과정입니다. 전처리 과정의 다양한 옵션들을 구성하기 위해 약간 복잡하게 되었지만,\n",
        "결과적으로는, 유저들의 특정한 아이템들을 따로 분리를 해서, 그 분리된 값을 모델이 예측할 수 있냐를 확인하기 위한 전처리 과정이라고 보시면 되겠습니다.\n",
        "실제로 나오는 데이터셋을 확인하면 더욱 이해가 빠를것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4.235294117647059"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a,b = 6800*600 + 600*600*4 + 600*200*2, 200*200 + 200*6800\n",
        "\n",
        "a/b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cgvNoy1Ybxa6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "import numpy as np\n",
        "\n",
        "def get_count(tp, id):\n",
        "    playcount_groupbyid = tp[[id]].groupby(id, as_index=False)\n",
        "    count = playcount_groupbyid.size()\n",
        "\n",
        "    return count\n",
        "\n",
        "# 특정한 횟수 이상의 리뷰가 존재하는(사용자의 경우 min_uc 이상, 아이템의 경우 min_sc이상)\n",
        "# 데이터만을 추출할 때 사용하는 함수입니다.\n",
        "# 현재 데이터셋에서는 결과적으로 원본그대로 사용하게 됩니다.\n",
        "def filter_triplets(tp, min_uc=5, min_sc=0):\n",
        "    if min_sc > 0:\n",
        "        itemcount = get_count(tp, 'item')\n",
        "        tp = tp[tp['item'].isin(itemcount[itemcount['size'] >= min_sc]['item'])]\n",
        "\n",
        "    if min_uc > 0:\n",
        "        usercount = get_count(tp, 'user')\n",
        "        tp = tp[tp['user'].isin(usercount[usercount['size'] >= min_uc]['user'])]\n",
        "\n",
        "    usercount, itemcount = get_count(tp, 'user'), get_count(tp, 'item')\n",
        "    return tp, usercount, itemcount\n",
        "\n",
        "#훈련된 모델을 이용해 검증할 데이터를 분리하는 함수입니다.\n",
        "#100개의 액션이 있다면, 그중에 test_prop 비율 만큼을 비워두고, 그것을 모델이 예측할 수 있는지를\n",
        "#확인하기 위함입니다.\n",
        "def split_train_test_proportion(data, test_prop=0.2):\n",
        "    data_grouped_by_user = data.groupby('user')\n",
        "    tr_list, te_list = list(), list()\n",
        "\n",
        "    np.random.seed(98765)\n",
        "\n",
        "    for _, group in data_grouped_by_user:\n",
        "        n_items_u = len(group)\n",
        "\n",
        "        if n_items_u >= 5:\n",
        "            idx = np.zeros(n_items_u, dtype='bool')\n",
        "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
        "\n",
        "            tr_list.append(group[np.logical_not(idx)])\n",
        "            te_list.append(group[idx])\n",
        "\n",
        "        else:\n",
        "            tr_list.append(group)\n",
        "\n",
        "    data_tr = pd.concat(tr_list)\n",
        "    data_te = pd.concat(te_list)\n",
        "\n",
        "    return data_tr, data_te\n",
        "\n",
        "def numerize(tp, profile2id, show2id):\n",
        "    uid = tp['user'].apply(lambda x: profile2id[x])\n",
        "    sid = tp['item'].apply(lambda x: show2id[x])\n",
        "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVFoRHrmVQsp",
        "outputId": "c96f119e-524e-4329-b89d-ed745e0eaf69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Load and Preprocess Movielens dataset\n",
            "원본 데이터\n",
            "            user   item        time\n",
            "0            11   4643  1230782529\n",
            "1            11    170  1230782534\n",
            "2            11    531  1230782539\n",
            "3            11    616  1230782542\n",
            "4            11   2140  1230782563\n",
            "...         ...    ...         ...\n",
            "5154466  138493  44022  1260209449\n",
            "5154467  138493   4958  1260209482\n",
            "5154468  138493  68319  1260209720\n",
            "5154469  138493  40819  1260209726\n",
            "5154470  138493  27311  1260209807\n",
            "\n",
            "[5154471 rows x 3 columns]\n",
            "5번 이상의 리뷰가 있는 유저들로만 구성된 데이터\n",
            "            user   item        time\n",
            "0            11   4643  1230782529\n",
            "1            11    170  1230782534\n",
            "2            11    531  1230782539\n",
            "3            11    616  1230782542\n",
            "4            11   2140  1230782563\n",
            "...         ...    ...         ...\n",
            "5154466  138493  44022  1260209449\n",
            "5154467  138493   4958  1260209482\n",
            "5154468  138493  68319  1260209720\n",
            "5154469  138493  40819  1260209726\n",
            "5154470  138493  27311  1260209807\n",
            "\n",
            "[5154471 rows x 3 columns]\n",
            "유저별 리뷰수\n",
            "          user  size\n",
            "0          11   376\n",
            "1          14   180\n",
            "2          18    77\n",
            "3          25    91\n",
            "4          31   154\n",
            "...       ...   ...\n",
            "31355  138473    63\n",
            "31356  138475   124\n",
            "31357  138486   137\n",
            "31358  138492    68\n",
            "31359  138493   314\n",
            "\n",
            "[31360 rows x 2 columns]\n",
            "아이템별 리뷰수\n",
            "         item   size\n",
            "0          1  12217\n",
            "1          2   3364\n",
            "2          3    734\n",
            "3          4     43\n",
            "4          5    590\n",
            "...      ...    ...\n",
            "6802  118700     54\n",
            "6803  118900     60\n",
            "6804  118997     52\n",
            "6805  119141    122\n",
            "6806  119145     78\n",
            "\n",
            "[6807 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "print(\"Load and Preprocess Movielens dataset\")\n",
        "# Load Data\n",
        "DATA_DIR = args.data\n",
        "raw_data = pd.read_csv(os.path.join(DATA_DIR, 'train_ratings.csv'), header=0)\n",
        "print(\"원본 데이터\\n\", raw_data)\n",
        "\n",
        "# Filter Data\n",
        "raw_data, user_activity, item_popularity = filter_triplets(raw_data, min_uc=5, min_sc=0)\n",
        "#제공된 훈련데이터의 유저는 모두 5개 이상의 리뷰가 있습니다.\n",
        "print(\"5번 이상의 리뷰가 있는 유저들로만 구성된 데이터\\n\",raw_data)\n",
        "\n",
        "print(\"유저별 리뷰수\\n\",user_activity)\n",
        "print(\"아이템별 리뷰수\\n\",item_popularity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.hist(np.log(raw_data.groupby('user').item.count()), bins=100)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "148.4131591025766"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.exp(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5154471, 3)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 381,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T1dTsWUrffP",
        "outputId": "6ceb7d17-d6e8-4a44-cafa-f3f84cadc6fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(BEFORE) unique_uid: 0            11\n",
            "1            14\n",
            "2            18\n",
            "3            25\n",
            "4            31\n",
            "          ...  \n",
            "31355    138473\n",
            "31356    138475\n",
            "31357    138486\n",
            "31358    138492\n",
            "31359    138493\n",
            "Name: user, Length: 31360, dtype: int64\n",
            "(AFTER) unique_uid: 6392      27968\n",
            "15438     67764\n",
            "603        2581\n",
            "18877     82969\n",
            "31197    137831\n",
            "          ...  \n",
            "30685    135379\n",
            "28589    125855\n",
            "9580      41891\n",
            "3571      15720\n",
            "3861      17029\n",
            "Name: user, Length: 31360, dtype: int64\n",
            "훈련 데이터에 사용될 사용자 수: 28360\n",
            "검증 데이터에 사용될 사용자 수: 0\n",
            "테스트 데이터에 사용될 사용자 수: 3000\n"
          ]
        }
      ],
      "source": [
        "# Shuffle User Indices\n",
        "unique_uid = user_activity.user # 아무리봐도 여기 이상한데?\n",
        "print(\"(BEFORE) unique_uid:\",unique_uid)\n",
        "np.random.seed(98765)\n",
        "idx_perm = np.random.permutation(unique_uid.size)\n",
        "unique_uid = unique_uid[idx_perm]\n",
        "print(\"(AFTER) unique_uid:\",unique_uid)\n",
        "\n",
        "n_users = unique_uid.size #31360\n",
        "n_heldout_users = 3000\n",
        "\n",
        "\n",
        "# Split Train/Validation/Test User Indices\n",
        "tr_users = unique_uid[:(n_users - n_heldout_users * 2)]\n",
        "vd_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)] # val 3000명\n",
        "te_users = unique_uid[(n_users - n_heldout_users):] # test 3000명\n",
        "\n",
        "tr_users = unique_uid[:(n_users - n_heldout_users)]\n",
        "vd_users = unique_uid[(n_users - n_heldout_users): (n_users - n_heldout_users)] # val 3000명\n",
        "\n",
        "#주의: 데이터의 수가 아닌 사용자의 수입니다!\n",
        "print(\"훈련 데이터에 사용될 사용자 수:\", len(tr_users))\n",
        "print(\"검증 데이터에 사용될 사용자 수:\", len(vd_users))\n",
        "print(\"테스트 데이터에 사용될 사용자 수:\", len(te_users))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'raw_data' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m raw_data\u001b[39m.\u001b[39minfo()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'raw_data' is not defined"
          ]
        }
      ],
      "source": [
        "raw_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 382,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yBsRCRqtPz6",
        "outputId": "fa16f7f7-09c9-42f3-b350-87168c60d12a"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "No objects to concatenate",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[382], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m vad_plays \u001b[39m=\u001b[39m raw_data\u001b[39m.\u001b[39mloc[raw_data[\u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39misin(vd_users)]\n\u001b[1;32m     22\u001b[0m vad_plays \u001b[39m=\u001b[39m vad_plays\u001b[39m.\u001b[39mloc[vad_plays[\u001b[39m'\u001b[39m\u001b[39mitem\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39misin(unique_sid)]\n\u001b[0;32m---> 23\u001b[0m vad_plays_tr, vad_plays_te \u001b[39m=\u001b[39m split_train_test_proportion(vad_plays)\n\u001b[1;32m     25\u001b[0m test_plays \u001b[39m=\u001b[39m raw_data\u001b[39m.\u001b[39mloc[raw_data[\u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39misin(te_users)]\n\u001b[1;32m     26\u001b[0m test_plays \u001b[39m=\u001b[39m test_plays\u001b[39m.\u001b[39mloc[test_plays[\u001b[39m'\u001b[39m\u001b[39mitem\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39misin(unique_sid)]\n",
            "Cell \u001b[0;32mIn[5], line 49\u001b[0m, in \u001b[0;36msplit_train_test_proportion\u001b[0;34m(data, test_prop)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m         tr_list\u001b[39m.\u001b[39mappend(group)\n\u001b[0;32m---> 49\u001b[0m data_tr \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat(tr_list)\n\u001b[1;32m     50\u001b[0m data_te \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(te_list)\n\u001b[1;32m     52\u001b[0m \u001b[39mreturn\u001b[39;00m data_tr, data_te\n",
            "File \u001b[0;32m~/miniconda3/envs/movie/lib/python3.10/site-packages/pandas/core/reshape/concat.py:380\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[39melif\u001b[39;00m copy \u001b[39mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    378\u001b[0m     copy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 380\u001b[0m op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[1;32m    381\u001b[0m     objs,\n\u001b[1;32m    382\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    383\u001b[0m     ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[1;32m    384\u001b[0m     join\u001b[39m=\u001b[39;49mjoin,\n\u001b[1;32m    385\u001b[0m     keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[1;32m    386\u001b[0m     levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[1;32m    387\u001b[0m     names\u001b[39m=\u001b[39;49mnames,\n\u001b[1;32m    388\u001b[0m     verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[1;32m    389\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    390\u001b[0m     sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    391\u001b[0m )\n\u001b[1;32m    393\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
            "File \u001b[0;32m~/miniconda3/envs/movie/lib/python3.10/site-packages/pandas/core/reshape/concat.py:443\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverify_integrity \u001b[39m=\u001b[39m verify_integrity\n\u001b[1;32m    441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy \u001b[39m=\u001b[39m copy\n\u001b[0;32m--> 443\u001b[0m objs, keys \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_clean_keys_and_objs(objs, keys)\n\u001b[1;32m    445\u001b[0m \u001b[39m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m    446\u001b[0m ndims \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_ndims(objs)\n",
            "File \u001b[0;32m~/miniconda3/envs/movie/lib/python3.10/site-packages/pandas/core/reshape/concat.py:505\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    502\u001b[0m     objs_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(objs)\n\u001b[1;32m    504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(objs_list) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 505\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo objects to concatenate\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    507\u001b[0m \u001b[39mif\u001b[39;00m keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m     objs_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(com\u001b[39m.\u001b[39mnot_none(\u001b[39m*\u001b[39mobjs_list))\n",
            "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
          ]
        }
      ],
      "source": [
        "##훈련 데이터에 해당하는 아이템들\n",
        "#Train는 25360명만 사용\n",
        "train_plays = raw_data.loc[raw_data['user'].isin(tr_users)] # index 아닌 user 번호 기준 검색 -> unique_uid user 번호로 바꿔야 해!\n",
        "\n",
        "##아이템 ID\n",
        "unique_sid = pd.unique(train_plays['item'])\n",
        "\n",
        "show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
        "profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid))\n",
        "\n",
        "pro_dir = os.path.join(DATA_DIR, 'pro_sg')\n",
        "\n",
        "if not os.path.exists(pro_dir):\n",
        "    os.makedirs(pro_dir)\n",
        "\n",
        "with open(os.path.join(pro_dir, 'unique_sid.txt'), 'w') as f:\n",
        "    for sid in unique_sid:\n",
        "        f.write('%s\\n' % sid)\n",
        "\n",
        "#Validation과 Test에는 input으로 사용될 tr 데이터와 정답을 확인하기 위한 te 데이터로 분리되었습니다.\n",
        "vad_plays = raw_data.loc[raw_data['user'].isin(vd_users)]\n",
        "vad_plays = vad_plays.loc[vad_plays['item'].isin(unique_sid)]\n",
        "vad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)\n",
        "\n",
        "test_plays = raw_data.loc[raw_data['user'].isin(te_users)]\n",
        "test_plays = test_plays.loc[test_plays['item'].isin(unique_sid)]\n",
        "test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)\n",
        "\n",
        "\n",
        "\n",
        "train_data = numerize(train_plays, profile2id, show2id)\n",
        "train_data.to_csv(os.path.join(pro_dir, 'train.csv'), index=False)\n",
        "\n",
        "\n",
        "vad_data_tr = numerize(vad_plays_tr, profile2id, show2id)\n",
        "vad_data_tr.to_csv(os.path.join(pro_dir, 'validation_tr.csv'), index=False)\n",
        "\n",
        "vad_data_te = numerize(vad_plays_te, profile2id, show2id)\n",
        "vad_data_te.to_csv(os.path.join(pro_dir, 'validation_te.csv'), index=False)\n",
        "\n",
        "test_data_tr = numerize(test_plays_tr, profile2id, show2id)\n",
        "test_data_tr.to_csv(os.path.join(pro_dir, 'test_tr.csv'), index=False)\n",
        "\n",
        "test_data_te = numerize(test_plays_te, profile2id, show2id)\n",
        "test_data_te.to_csv(os.path.join(pro_dir, 'test_te.csv'), index=False)\n",
        "\n",
        "inf_data = raw_data[['user','item']].copy()\n",
        "inf_data = numerize(inf_data, profile2id, show2id)\n",
        "inf_data.to_csv(os.path.join(pro_dir, 'inference.csv'), index=False)\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 383,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkdg2OkjqVUM",
        "outputId": "28e101f0-e9ed-474b-8f3b-1fc161872ebf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (0, 17)\t1.0\n",
            "  (0, 40)\t1.0\n",
            "  (0, 41)\t1.0\n",
            "  (0, 46)\t1.0\n",
            "  (0, 70)\t1.0\n",
            "  (0, 78)\t1.0\n",
            "  (0, 79)\t1.0\n",
            "  (0, 80)\t1.0\n",
            "  (0, 82)\t1.0\n",
            "  (0, 85)\t1.0\n",
            "  (0, 198)\t1.0\n",
            "  (0, 200)\t1.0\n",
            "  (0, 233)\t1.0\n",
            "  (0, 260)\t1.0\n",
            "  (0, 264)\t1.0\n",
            "  (0, 266)\t1.0\n",
            "  (0, 269)\t1.0\n",
            "  (0, 270)\t1.0\n",
            "  (0, 271)\t1.0\n",
            "  (0, 272)\t1.0\n",
            "  (0, 285)\t1.0\n",
            "  (0, 287)\t1.0\n",
            "  (0, 292)\t1.0\n",
            "  (0, 294)\t1.0\n",
            "  (0, 308)\t1.0\n",
            "  :\t:\n",
            "  (25359, 1429)\t1.0\n",
            "  (25359, 1447)\t1.0\n",
            "  (25359, 1794)\t1.0\n",
            "  (25359, 1872)\t1.0\n",
            "  (25359, 2436)\t1.0\n",
            "  (25359, 2669)\t1.0\n",
            "  (25359, 2776)\t1.0\n",
            "  (25359, 2777)\t1.0\n",
            "  (25359, 2860)\t1.0\n",
            "  (25359, 2882)\t1.0\n",
            "  (25359, 3041)\t1.0\n",
            "  (25359, 3063)\t1.0\n",
            "  (25359, 3101)\t1.0\n",
            "  (25359, 3247)\t1.0\n",
            "  (25359, 3340)\t1.0\n",
            "  (25359, 3886)\t1.0\n",
            "  (25359, 3956)\t1.0\n",
            "  (25359, 4293)\t1.0\n",
            "  (25359, 4328)\t1.0\n",
            "  (25359, 4492)\t1.0\n",
            "  (25359, 4606)\t1.0\n",
            "  (25359, 4915)\t1.0\n",
            "  (25359, 4946)\t1.0\n",
            "  (25359, 5026)\t1.0\n",
            "  (25359, 6637)\t1.0\n",
            "  (0, 17)\t1.0\n",
            "  (0, 46)\t1.0\n",
            "  (0, 79)\t1.0\n",
            "  (0, 82)\t1.0\n",
            "  (0, 116)\t1.0\n",
            "  (0, 149)\t1.0\n",
            "  (0, 159)\t1.0\n",
            "  (0, 196)\t1.0\n",
            "  (0, 198)\t1.0\n",
            "  (0, 207)\t1.0\n",
            "  (0, 209)\t1.0\n",
            "  (0, 260)\t1.0\n",
            "  (0, 266)\t1.0\n",
            "  (0, 270)\t1.0\n",
            "  (0, 289)\t1.0\n",
            "  (0, 292)\t1.0\n",
            "  (0, 358)\t1.0\n",
            "  (0, 362)\t1.0\n",
            "  (0, 366)\t1.0\n",
            "  (0, 375)\t1.0\n",
            "  (0, 591)\t1.0\n",
            "  (0, 639)\t1.0\n",
            "  (0, 645)\t1.0\n",
            "  (0, 718)\t1.0\n",
            "  (0, 774)\t1.0\n",
            "  :\t:\n",
            "  (2999, 833)\t1.0\n",
            "  (2999, 943)\t1.0\n",
            "  (2999, 946)\t1.0\n",
            "  (2999, 972)\t1.0\n",
            "  (2999, 993)\t1.0\n",
            "  (2999, 1040)\t1.0\n",
            "  (2999, 1050)\t1.0\n",
            "  (2999, 1062)\t1.0\n",
            "  (2999, 1144)\t1.0\n",
            "  (2999, 1158)\t1.0\n",
            "  (2999, 1235)\t1.0\n",
            "  (2999, 1248)\t1.0\n",
            "  (2999, 1287)\t1.0\n",
            "  (2999, 1302)\t1.0\n",
            "  (2999, 1338)\t1.0\n",
            "  (2999, 1398)\t1.0\n",
            "  (2999, 1504)\t1.0\n",
            "  (2999, 2003)\t1.0\n",
            "  (2999, 2165)\t1.0\n",
            "  (2999, 3378)\t1.0\n",
            "  (2999, 3406)\t1.0\n",
            "  (2999, 4199)\t1.0\n",
            "  (2999, 4536)\t1.0\n",
            "  (2999, 5275)\t1.0\n",
            "  (2999, 5503)\t1.0\n",
            "  (0, 210)\t1.0\n",
            "  (0, 560)\t1.0\n",
            "  (0, 685)\t1.0\n",
            "  (0, 690)\t1.0\n",
            "  (0, 711)\t1.0\n",
            "  (0, 935)\t1.0\n",
            "  (0, 1040)\t1.0\n",
            "  (0, 1154)\t1.0\n",
            "  (0, 3123)\t1.0\n",
            "  (1, 86)\t1.0\n",
            "  (1, 116)\t1.0\n",
            "  (1, 148)\t1.0\n",
            "  (1, 159)\t1.0\n",
            "  (1, 186)\t1.0\n",
            "  (1, 207)\t1.0\n",
            "  (1, 264)\t1.0\n",
            "  (1, 270)\t1.0\n",
            "  (1, 283)\t1.0\n",
            "  (1, 292)\t1.0\n",
            "  (1, 300)\t1.0\n",
            "  (1, 306)\t1.0\n",
            "  (1, 359)\t1.0\n",
            "  (1, 482)\t1.0\n",
            "  (1, 632)\t1.0\n",
            "  (1, 1526)\t1.0\n",
            "  :\t:\n",
            "  (2997, 853)\t1.0\n",
            "  (2997, 1150)\t1.0\n",
            "  (2997, 1451)\t1.0\n",
            "  (2998, 41)\t1.0\n",
            "  (2998, 82)\t1.0\n",
            "  (2998, 144)\t1.0\n",
            "  (2998, 265)\t1.0\n",
            "  (2998, 362)\t1.0\n",
            "  (2998, 566)\t1.0\n",
            "  (2998, 782)\t1.0\n",
            "  (2998, 1016)\t1.0\n",
            "  (2998, 1916)\t1.0\n",
            "  (2998, 2212)\t1.0\n",
            "  (2999, 277)\t1.0\n",
            "  (2999, 312)\t1.0\n",
            "  (2999, 462)\t1.0\n",
            "  (2999, 483)\t1.0\n",
            "  (2999, 662)\t1.0\n",
            "  (2999, 810)\t1.0\n",
            "  (2999, 952)\t1.0\n",
            "  (2999, 1015)\t1.0\n",
            "  (2999, 1369)\t1.0\n",
            "  (2999, 2175)\t1.0\n",
            "  (2999, 2188)\t1.0\n",
            "  (2999, 3380)\t1.0\n"
          ]
        }
      ],
      "source": [
        "#데이터 셋 확인\n",
        "print(train_data)\n",
        "print(vad_data_tr)\n",
        "print(vad_data_te)\n",
        "# print(test_data_tr)\n",
        "# print(test_data_te)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMiq9leyWWL1"
      },
      "source": [
        "##3. 데이터 로더 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 384,
      "metadata": {
        "id": "nxUADr9ibxa8"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DataLoader(): # 존재하지도 않는 user가 sparse matrix 상에 포함됨\n",
        "    '''\n",
        "    Load Movielens dataset\n",
        "    '''\n",
        "    def __init__(self, path):\n",
        "\n",
        "        self.pro_dir = os.path.join(path, 'pro_sg')\n",
        "        assert os.path.exists(self.pro_dir), \"Preprocessed files do not exist. Run data.py\"\n",
        "\n",
        "        self.n_items = self.load_n_items()\n",
        "\n",
        "    def load_data(self, datatype='train'):\n",
        "        if datatype == 'train':\n",
        "            return self._load_train_data()\n",
        "        elif datatype == 'validation':\n",
        "            return self._load_tr_te_data(datatype)\n",
        "        elif datatype == 'test':\n",
        "            return self._load_tr_te_data(datatype)\n",
        "        elif datatype == 'inference':\n",
        "            return self._load_inf_data(datatype)\n",
        "        else:\n",
        "            raise ValueError(\"datatype should be in [train, validation, test, inference]\")\n",
        "\n",
        "    def load_n_items(self):\n",
        "        unique_sid = list()\n",
        "        with open(os.path.join(self.pro_dir, 'unique_sid.txt'), 'r') as f:\n",
        "            for line in f:\n",
        "                unique_sid.append(line.strip())\n",
        "        n_items = len(unique_sid)\n",
        "        return n_items\n",
        "\n",
        "    def _load_train_data(self):\n",
        "        path = os.path.join(self.pro_dir, 'train.csv')\n",
        "\n",
        "        tp = pd.read_csv(path)\n",
        "        n_users = tp['uid'].max() + 1\n",
        "\n",
        "        rows, cols = tp['uid'], tp['sid']\n",
        "        data = sparse.csr_matrix((np.ones_like(rows),\n",
        "                                 (rows, cols)), dtype='float64',\n",
        "                                 shape=(n_users, self.n_items)) # uid,sid에만 1 채우고 나머지는 모두 0을 채우는 sparse interaction matrix\n",
        "        return data\n",
        "\n",
        "    def _load_tr_te_data(self, datatype='test'):\n",
        "        tr_path = os.path.join(self.pro_dir, '{}_tr.csv'.format(datatype))\n",
        "        te_path = os.path.join(self.pro_dir, '{}_te.csv'.format(datatype))\n",
        "\n",
        "        tp_tr = pd.read_csv(tr_path)\n",
        "        tp_te = pd.read_csv(te_path)\n",
        "\n",
        "        start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
        "        end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
        "\n",
        "        rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
        "        rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
        "\n",
        "        data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
        "                                    (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, self.n_items))\n",
        "        data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
        "                                    (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, self.n_items))\n",
        "        \n",
        "        return data_tr, data_te\n",
        "    \n",
        "    def _load_inf_data(self, datatype='inference'):\n",
        "        inf_path = os.path.join(self.pro_dir, 'inference.csv')\n",
        "\n",
        "        data_inf = pd.read_csv(inf_path)\n",
        "\n",
        "        n_rows = data_inf.uid.nunique()\n",
        "        n_cols = data_inf.sid.nunique()\n",
        "\n",
        "        rows = data_inf['uid']\n",
        "        cols = data_inf['sid']\n",
        "\n",
        "        data_inf = sparse.csr_matrix((np.ones_like(rows),\n",
        "                                    (rows, cols)), dtype='float64', shape=(n_rows, n_cols))\n",
        "\n",
        "        return data_inf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FHhwKqXWaUZ"
      },
      "source": [
        "## 4. 모델정의\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 409,
      "metadata": {
        "id": "QYlGPJTYU0ii"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#이미 완성된 MultiDAE의 코드를 참고하여 그 아래 MultiVAE의 코드를 완성해보세요!\n",
        "class MultiDAE(nn.Module):\n",
        "    \"\"\"\n",
        "    Container module for Multi-DAE.\n",
        "\n",
        "    Multi-DAE : Denoising Autoencoder with Multinomial Likelihood\n",
        "    See Variational Autoencoders for Collaborative Filtering\n",
        "    https://arxiv.org/abs/1802.05814\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p_dims, q_dims=None, dropout=0.5):\n",
        "        super(MultiDAE, self).__init__()\n",
        "        self.p_dims = p_dims\n",
        "        if q_dims:\n",
        "            assert q_dims[0] == p_dims[-1], \"In and Out dimensions must equal to each other\"\n",
        "            assert q_dims[-1] == p_dims[0], \"Latent dimension for p- and q- network mismatches.\"\n",
        "            self.q_dims = q_dims\n",
        "        else:\n",
        "            self.q_dims = p_dims[::-1]\n",
        "\n",
        "        self.dims = self.q_dims + self.p_dims[1:]\n",
        "        self.layers = nn.ModuleList([nn.Linear(d_in, d_out) for\n",
        "            d_in, d_out in zip(self.dims[:-1], self.dims[1:])])\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input):\n",
        "        # h = F.normalize(input)\n",
        "        h = self.drop(input)\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            h = layer(h)\n",
        "            if i != len(self.layers) - 1:\n",
        "                h = F.tanh(h)\n",
        "        return h\n",
        "\n",
        "    def init_weights(self):\n",
        "        for layer in self.layers:\n",
        "            # Xavier Initialization for weights\n",
        "            size = layer.weight.size()\n",
        "            fan_out = size[0]\n",
        "            fan_in = size[1]\n",
        "            std = np.sqrt(2.0/(fan_in + fan_out))\n",
        "            layer.weight.data.normal_(0.0, std)\n",
        "\n",
        "            # Normal Initialization for Biases\n",
        "            layer.bias.data.normal_(0.0, 0.001)\n",
        "\n",
        "\n",
        "\n",
        "class MultiVAE(nn.Module):\n",
        "    \"\"\"\n",
        "    Container module for Multi-VAE.\n",
        "\n",
        "    Multi-VAE : Variational Autoencoder with Multinomial Likelihood\n",
        "    See Variational Autoencoders for Collaborative Filtering\n",
        "    https://arxiv.org/abs/1802.05814\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p_dims, q_dims=None, dropout=0.5):\n",
        "        super(MultiVAE, self).__init__()\n",
        "        self.p_dims = p_dims\n",
        "        if q_dims:\n",
        "            assert q_dims[0] == p_dims[-1], \"In and Out dimensions must equal to each other\"\n",
        "            assert q_dims[-1] == p_dims[0], \"Latent dimension for p- and q- network mismatches.\"\n",
        "            self.q_dims = q_dims\n",
        "        else:\n",
        "            self.q_dims = p_dims[::-1] # default: 대칭 구조\n",
        "\n",
        "        # Last dimension of q- network is for mean and variance\n",
        "        temp_q_dims = self.q_dims[:-1] + [self.q_dims[-1] * 2]\n",
        "        self.q_layers = nn.ModuleList([nn.Linear(d_in, d_out) for\n",
        "            d_in, d_out in zip(temp_q_dims[:-1], temp_q_dims[1:])])\n",
        "        self.p_layers = nn.ModuleList([nn.Linear(d_in, d_out) for\n",
        "            d_in, d_out in zip(self.p_dims[:-1], self.p_dims[1:])])\n",
        "\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input):\n",
        "        mu, logvar = self.encode(input)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "    def encode(self, input):\n",
        "        # h = F.normalize(input)\n",
        "        h = self.drop(input)\n",
        "\n",
        "        for i, layer in enumerate(self.q_layers):\n",
        "            h = layer(h)\n",
        "            if i != len(self.q_layers) - 1:\n",
        "                h = F.tanh(h)\n",
        "            else: #\n",
        "                mu = h[:, :self.q_dims[-1]]\n",
        "                logvar = h[:, self.q_dims[-1]:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar): # latent factor(모수) 조정\n",
        "        if self.training:\n",
        "            std = torch.exp(0.5 * logvar)\n",
        "            eps = torch.randn_like(std) # std와 동일 모양의 N(0,1)난수생성\n",
        "            return eps.mul(std).add_(mu) # train: 난수*std + mu\n",
        "        else:\n",
        "            return mu # val/test: mu\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = z\n",
        "        for i, layer in enumerate(self.p_layers):\n",
        "            h = layer(h)\n",
        "            if i != len(self.p_layers) - 1:\n",
        "                h = F.tanh(h)\n",
        "        return h\n",
        "\n",
        "    def init_weights(self):\n",
        "        for layer in self.q_layers:\n",
        "            # Xavier Initialization for weights\n",
        "            size = layer.weight.size()\n",
        "            fan_out = size[0]\n",
        "            fan_in = size[1]\n",
        "            std = np.sqrt(2.0/(fan_in + fan_out))\n",
        "            layer.weight.data.normal_(0.0, std)\n",
        "\n",
        "            # Normal Initialization for Biases\n",
        "            layer.bias.data.normal_(0.0, 0.001)\n",
        "\n",
        "        for layer in self.p_layers:\n",
        "            # Xavier Initialization for weights\n",
        "            size = layer.weight.size()\n",
        "            fan_out = size[0]\n",
        "            fan_in = size[1]\n",
        "            std = np.sqrt(2.0/(fan_in + fan_out))\n",
        "            layer.weight.data.normal_(0.0, std)\n",
        "\n",
        "            # Normal Initialization for Biases\n",
        "            layer.bias.data.normal_(0.0, 0.001)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def loss_function_vae(recon_x, x, mu, logvar, anneal=1.0):\n",
        "    BCE = -torch.mean(torch.sum(F.log_softmax(recon_x, 1) * x, -1)) # softmax: multinomial의 loss\n",
        "    # BCE = torch.nn.BCEWithLogitsLoss(reduction='sum')(recon_x, x)\n",
        "    KLD = -0.5 * torch.mean(torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)) # \n",
        "\n",
        "    return BCE + anneal * KLD\n",
        "\n",
        "def loss_function_dae(recon_x, x):\n",
        "    BCE = -torch.mean(torch.sum(F.log_softmax(recon_x, 1) * x, -1))\n",
        "    # BCE = torch.nn.BCEWithLogitsLoss(reduction='sum')(recon_x, x)\n",
        "    return BCE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 398,
      "metadata": {
        "id": "7nEfVTktbxa8"
      },
      "outputs": [],
      "source": [
        "\n",
        "def sparse2torch_sparse(data): # encoder F.normalize에서 대신 활용\n",
        "    \"\"\"\n",
        "    Convert scipy sparse matrix to torch sparse tensor with L2 Normalization\n",
        "    This is much faster than naive use of torch.FloatTensor(data.toarray())\n",
        "    https://discuss.pytorch.org/t/sparse-tensor-use-cases/22047/2\n",
        "    \"\"\"\n",
        "    samples = data.shape[0] # n_users\n",
        "    features = data.shape[1] # n_items\n",
        "    coo_data = data.tocoo()\n",
        "    indices = torch.LongTensor([coo_data.row, coo_data.col])\n",
        "    row_norms_inv = 1 / np.sqrt(data.sum(1)) # 유저별로 interaction 수만큼 normalize\n",
        "    # row_norms_inv = 1 / np.linalg.norm(data, axis=1) # 유저별로 interaction 수만큼 normalize\n",
        "    row2val = {i : row_norms_inv[i].item() for i in range(samples)}\n",
        "    values = np.array([row2val[r] for r in coo_data.row])\n",
        "    t = torch.sparse.FloatTensor(indices, torch.from_numpy(values).float(), [samples, features])\n",
        "    return torch.FloatTensor(t.to_dense())\n",
        "\n",
        "def naive_sparse2tensor(data):\n",
        "    return torch.FloatTensor(data.toarray())\n",
        "\n",
        "\n",
        "def train(model, criterion, optimizer, is_VAE = False):\n",
        "    # Turn on training mode\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    start_time = time.time()\n",
        "    global update_count\n",
        "\n",
        "    np.random.shuffle(idxlist)\n",
        "\n",
        "    for batch_idx, start_idx in enumerate(range(0, N, args.batch_size)):\n",
        "        end_idx = min(start_idx + args.batch_size, N)\n",
        "        data = train_data[idxlist[start_idx:end_idx]] # 여기의 train_data: sparse interaction matrix 형태\n",
        "        data = naive_sparse2tensor(data).to(device)\n",
        "        # data = sparse2torch_sparse(data).to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if is_VAE:\n",
        "          if args.total_anneal_steps > 0:\n",
        "            anneal = min(args.anneal_cap,\n",
        "                            1. * update_count / args.total_anneal_steps)\n",
        "          else:\n",
        "              anneal = args.anneal_cap\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          recon_batch, mu, logvar = model(data)\n",
        "\n",
        "          loss = criterion(recon_batch, data, mu, logvar, anneal)\n",
        "        else:\n",
        "          recon_batch = model(data)\n",
        "          loss = criterion(recon_batch, data)\n",
        "\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "        update_count += 1\n",
        "\n",
        "        if batch_idx % args.log_interval == 0 and batch_idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:4d}/{:4d} batches | ms/batch {:4.2f} | '\n",
        "                    'loss {:4.2f}'.format(\n",
        "                        epoch, batch_idx, len(range(0, N, args.batch_size)),\n",
        "                        elapsed * 1000 / args.log_interval,\n",
        "                        train_loss / args.log_interval))\n",
        "\n",
        "\n",
        "            start_time = time.time()\n",
        "            train_loss = 0.0\n",
        "\n",
        "\n",
        "def evaluate(model, criterion, data_tr, data_te, is_VAE=False):\n",
        "    # Turn on evaluation mode\n",
        "    model.eval()\n",
        "    global update_count\n",
        "    e_idxlist = list(range(data_tr.shape[0]))\n",
        "    e_N = data_tr.shape[0]\n",
        "    total_val_loss_list = []\n",
        "    n100_list = []\n",
        "    r20_list = []\n",
        "    r50_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for start_idx in range(0, e_N, args.batch_size):\n",
        "            end_idx = min(start_idx + args.batch_size, N)\n",
        "            data = data_tr[e_idxlist[start_idx:end_idx]]\n",
        "            heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
        "\n",
        "            data_tensor = naive_sparse2tensor(data).to(device)\n",
        "            # data_tensor = sparse2torch_sparse(data).to(device)\n",
        "            if is_VAE :\n",
        "\n",
        "              if args.total_anneal_steps > 0:\n",
        "                  anneal = min(args.anneal_cap,\n",
        "                                1. * update_count / args.total_anneal_steps)\n",
        "              else:\n",
        "                  anneal = args.anneal_cap\n",
        "\n",
        "              recon_batch, mu, logvar = model(data_tensor)\n",
        "\n",
        "              loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
        "\n",
        "            else :\n",
        "              recon_batch = model(data_tensor)\n",
        "              loss = criterion(recon_batch, data_tensor)\n",
        "\n",
        "            total_val_loss_list.append(loss.item())\n",
        "\n",
        "            # Exclude examples from training set\n",
        "            recon_batch = recon_batch.cpu().numpy()\n",
        "            recon_batch[data.nonzero()] = -np.inf\n",
        "\n",
        "            n100 = NDCG_binary_at_k_batch(recon_batch, heldout_data, 100)\n",
        "            r20 = Recall_at_k_batch(recon_batch, heldout_data, 20)\n",
        "            r50 = Recall_at_k_batch(recon_batch, heldout_data, 50)\n",
        "            # top10 = torch.topk(recon_batch)\n",
        "\n",
        "            n100_list.append(n100)\n",
        "            r20_list.append(r20)\n",
        "            r50_list.append(r50)\n",
        "\n",
        "    n100_list = np.concatenate(n100_list)\n",
        "    r20_list = np.concatenate(r20_list)\n",
        "    r50_list = np.concatenate(r50_list)\n",
        "\n",
        "    return np.nanmean(total_val_loss_list), np.nanmean(n100_list), np.nanmean(r20_list), np.nanmean(r50_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOsCJbb_X9gl"
      },
      "source": [
        "## Metric 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 399,
      "metadata": {
        "id": "zxNtit6vbxa-"
      },
      "outputs": [],
      "source": [
        "import bottleneck as bn\n",
        "import numpy as np\n",
        "\n",
        "def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n",
        "    '''\n",
        "    Normalized Discounted Cumulative Gain@k for binary relevance\n",
        "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
        "    '''\n",
        "    batch_users = X_pred.shape[0]\n",
        "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
        "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
        "                       idx_topk_part[:, :k]]\n",
        "    idx_part = np.argsort(-topk_part, axis=1)\n",
        "\n",
        "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
        "\n",
        "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
        "\n",
        "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
        "                         idx_topk].toarray() * tp).sum(axis=1)\n",
        "    IDCG = np.array([(tp[:min(n, k)]).sum()\n",
        "                     for n in heldout_batch.getnnz(axis=1)])\n",
        "    return DCG / IDCG\n",
        "\n",
        "\n",
        "def Recall_at_k_batch(X_pred, heldout_batch, k=100):\n",
        "    batch_users = X_pred.shape[0]\n",
        "\n",
        "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
        "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
        "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
        "\n",
        "    X_true_binary = (heldout_batch > 0).toarray()\n",
        "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(\n",
        "        np.float32)\n",
        "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
        "    return recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDD7lD7sHcnH"
      },
      "source": [
        "## MultiDAE 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 400,
      "metadata": {
        "id": "WLYyTwToX4fm"
      },
      "outputs": [],
      "source": [
        "\n",
        "###############################################################################\n",
        "# Load data\n",
        "###############################################################################\n",
        "\n",
        "loader = DataLoader(args.data)\n",
        "\n",
        "n_items = loader.load_n_items()\n",
        "train_data = loader.load_data('train')\n",
        "vad_data_tr, vad_data_te = loader.load_data('validation')\n",
        "test_data_tr, test_data_te = loader.load_data('test')\n",
        "\n",
        "N = train_data.shape[0]\n",
        "idxlist = list(range(N))\n",
        "\n",
        "###############################################################################\n",
        "# Build the model\n",
        "###############################################################################\n",
        "\n",
        "p_dims = [200, 600, n_items]\n",
        "model = MultiDAE(p_dims).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
        "criterion = loss_function_dae\n",
        "\n",
        "###############################################################################\n",
        "# Training code\n",
        "###############################################################################\n",
        "\n",
        "best_n100 = -np.inf\n",
        "update_count = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 401,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rOEDs2Lbxa-",
        "outputId": "ccbcf453-8276-498e-85d8-f44a83687c6a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/ml/miniconda3/envs/movie/lib/python3.10/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 1.66s | valid loss 259683.58 | n100 0.262 | r20 0.192 | r50 0.238\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 1.50s | valid loss 253584.77 | n100 0.262 | r20 0.191 | r50 0.238\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 1.53s | valid loss 236254.58 | n100 0.263 | r20 0.193 | r50 0.240\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 1.53s | valid loss 231153.55 | n100 0.264 | r20 0.191 | r50 0.240\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 1.62s | valid loss 229580.88 | n100 0.264 | r20 0.194 | r50 0.241\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 1.60s | valid loss 224403.30 | n100 0.274 | r20 0.202 | r50 0.251\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 1.61s | valid loss 222482.88 | n100 0.280 | r20 0.207 | r50 0.257\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 1.56s | valid loss 216268.48 | n100 0.304 | r20 0.229 | r50 0.278\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 1.56s | valid loss 212178.99 | n100 0.311 | r20 0.234 | r50 0.285\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 1.64s | valid loss 208338.39 | n100 0.324 | r20 0.244 | r50 0.297\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 1.63s | valid loss 205906.56 | n100 0.330 | r20 0.247 | r50 0.301\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 1.65s | valid loss 204463.88 | n100 0.334 | r20 0.251 | r50 0.306\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 1.62s | valid loss 202732.46 | n100 0.339 | r20 0.255 | r50 0.310\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 1.62s | valid loss 202766.08 | n100 0.342 | r20 0.258 | r50 0.314\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 1.63s | valid loss 198696.98 | n100 0.349 | r20 0.263 | r50 0.319\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 1.62s | valid loss 196021.28 | n100 0.359 | r20 0.273 | r50 0.329\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 1.68s | valid loss 194700.95 | n100 0.366 | r20 0.279 | r50 0.334\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 1.68s | valid loss 193072.54 | n100 0.371 | r20 0.282 | r50 0.339\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 1.62s | valid loss 192554.23 | n100 0.374 | r20 0.285 | r50 0.342\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 1.65s | valid loss 190882.79 | n100 0.378 | r20 0.288 | r50 0.345\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 1.64s | valid loss 190004.75 | n100 0.382 | r20 0.292 | r50 0.350\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 1.65s | valid loss 188671.34 | n100 0.388 | r20 0.297 | r50 0.355\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 1.66s | valid loss 187192.39 | n100 0.390 | r20 0.299 | r50 0.357\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 1.65s | valid loss 186746.80 | n100 0.391 | r20 0.299 | r50 0.358\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 1.74s | valid loss 186610.84 | n100 0.395 | r20 0.302 | r50 0.360\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 1.68s | valid loss 185055.19 | n100 0.396 | r20 0.303 | r50 0.363\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 1.59s | valid loss 183851.92 | n100 0.400 | r20 0.307 | r50 0.367\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 1.63s | valid loss 184604.08 | n100 0.401 | r20 0.307 | r50 0.369\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 1.58s | valid loss 183167.09 | n100 0.403 | r20 0.308 | r50 0.369\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 1.71s | valid loss 182759.10 | n100 0.407 | r20 0.312 | r50 0.373\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 1.64s | valid loss 182004.63 | n100 0.408 | r20 0.314 | r50 0.373\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 1.63s | valid loss 181693.41 | n100 0.408 | r20 0.315 | r50 0.375\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 1.73s | valid loss 180374.49 | n100 0.411 | r20 0.317 | r50 0.377\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 1.63s | valid loss 179890.89 | n100 0.412 | r20 0.317 | r50 0.378\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 1.65s | valid loss 180018.95 | n100 0.415 | r20 0.320 | r50 0.380\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 1.59s | valid loss 178838.96 | n100 0.417 | r20 0.323 | r50 0.381\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 1.64s | valid loss 179175.21 | n100 0.418 | r20 0.325 | r50 0.382\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 1.63s | valid loss 179060.30 | n100 0.420 | r20 0.324 | r50 0.383\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 1.67s | valid loss 179133.11 | n100 0.422 | r20 0.327 | r50 0.385\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 1.66s | valid loss 177047.12 | n100 0.423 | r20 0.327 | r50 0.386\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 1.61s | valid loss 177813.74 | n100 0.426 | r20 0.331 | r50 0.388\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 1.70s | valid loss 176721.97 | n100 0.427 | r20 0.331 | r50 0.389\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 1.72s | valid loss 175898.08 | n100 0.427 | r20 0.331 | r50 0.389\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 1.74s | valid loss 175241.71 | n100 0.428 | r20 0.331 | r50 0.390\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 1.59s | valid loss 174903.49 | n100 0.428 | r20 0.332 | r50 0.390\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 1.59s | valid loss 174809.21 | n100 0.429 | r20 0.333 | r50 0.391\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 1.67s | valid loss 174592.35 | n100 0.431 | r20 0.333 | r50 0.393\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 1.74s | valid loss 173711.42 | n100 0.431 | r20 0.334 | r50 0.393\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 1.71s | valid loss 174802.64 | n100 0.432 | r20 0.334 | r50 0.393\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 1.65s | valid loss 174309.80 | n100 0.432 | r20 0.336 | r50 0.394\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time: 1.62s | valid loss 173718.97 | n100 0.433 | r20 0.336 | r50 0.393\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time: 1.60s | valid loss 174163.81 | n100 0.434 | r20 0.337 | r50 0.396\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time: 1.63s | valid loss 172252.99 | n100 0.435 | r20 0.337 | r50 0.396\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time: 1.70s | valid loss 172633.96 | n100 0.434 | r20 0.336 | r50 0.396\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time: 1.71s | valid loss 174462.39 | n100 0.435 | r20 0.337 | r50 0.396\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time: 1.63s | valid loss 173156.63 | n100 0.438 | r20 0.340 | r50 0.398\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time: 1.64s | valid loss 172578.28 | n100 0.436 | r20 0.340 | r50 0.399\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time: 1.59s | valid loss 172582.96 | n100 0.438 | r20 0.340 | r50 0.399\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time: 1.66s | valid loss 171257.50 | n100 0.437 | r20 0.339 | r50 0.400\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss 168874.17 | n100 0.44 | r20 0.34 | r50 0.40\n",
            "=========================================================================================\n",
            "best epoch: 58\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, args.epochs+40):\n",
        "    epoch_start_time = time.time()\n",
        "    train(model, criterion, optimizer, is_VAE=False)\n",
        "    val_loss, n100, r20, r50 = evaluate(model, criterion, vad_data_tr, vad_data_te, is_VAE=False)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:4.2f}s | valid loss {:4.2f} | '\n",
        "            'n100 {:5.3f} | r20 {:5.3f} | r50 {:5.3f}'.format(\n",
        "                epoch, time.time() - epoch_start_time, val_loss,\n",
        "                n100, r20, r50))\n",
        "    print('-' * 89)\n",
        "\n",
        "    n_iter = epoch * len(range(0, N, args.batch_size))\n",
        "\n",
        "\n",
        "    # Save the model if the n100 is the best we've seen so far.\n",
        "    if n100 > best_n100:\n",
        "        with open(args.save, 'wb') as f:\n",
        "            torch.save(model, f)\n",
        "        best_n100 = n100\n",
        "        best_epoch = epoch\n",
        "\n",
        "\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(args.save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss, n100, r20, r50 = evaluate(model, criterion, test_data_tr, test_data_te, is_VAE=False)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:4.2f} | n100 {:4.2f} | r20 {:4.2f} | '\n",
        "        'r50 {:4.2f}'.format(test_loss, n100, r20, r50))\n",
        "print('=' * 89)\n",
        "print('best epoch:', best_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1QjCbMBXw4v"
      },
      "source": [
        "## MultiVAE 테스트 (TODO)\n",
        "\n",
        "위의 MultiVAE 모델 코드, train, evaluate 함수를 완성하여, 아래 훈련 코드가 정상적으로 동작하도록 해보세요!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 402,
      "metadata": {
        "id": "78zFFNzgbxa_"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# Load data\n",
        "###############################################################################\n",
        "\n",
        "loader = DataLoader(args.data)\n",
        "\n",
        "n_items = loader.load_n_items()\n",
        "train_data = loader.load_data('train')\n",
        "vad_data_tr, vad_data_te = loader.load_data('validation')\n",
        "test_data_tr, test_data_te = loader.load_data('test')\n",
        "\n",
        "N = train_data.shape[0] # n_users\n",
        "idxlist = list(range(N)) # 유저들 묶음을 batch로\n",
        "\n",
        "###############################################################################\n",
        "# Build the model\n",
        "###############################################################################\n",
        "\n",
        "p_dims = [200, 600, n_items]\n",
        "model = MultiVAE(p_dims).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=args.wd)\n",
        "criterion = loss_function_vae\n",
        "\n",
        "###############################################################################\n",
        "# Training code\n",
        "###############################################################################\n",
        "\n",
        "best_n100 = -np.inf\n",
        "update_count = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 403,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoUFwndCvvtp",
        "outputId": "3ec0ad84-6ac1-43f7-9ad1-8de61e1307d4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/ml/miniconda3/envs/movie/lib/python3.10/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 1.68s | valid loss 257870.44 | n100 0.259 | r20 0.191 | r50 0.236\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 1.57s | valid loss 244882.44 | n100 0.262 | r20 0.189 | r50 0.238\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 1.61s | valid loss 237222.62 | n100 0.260 | r20 0.190 | r50 0.239\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 1.55s | valid loss 234359.27 | n100 0.263 | r20 0.190 | r50 0.239\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 1.66s | valid loss 232969.33 | n100 0.263 | r20 0.192 | r50 0.239\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 1.62s | valid loss 231792.95 | n100 0.262 | r20 0.193 | r50 0.240\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 1.60s | valid loss 229369.92 | n100 0.264 | r20 0.195 | r50 0.241\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 1.57s | valid loss 227265.66 | n100 0.270 | r20 0.201 | r50 0.248\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 1.51s | valid loss 226283.13 | n100 0.273 | r20 0.201 | r50 0.250\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 1.57s | valid loss 224913.71 | n100 0.277 | r20 0.204 | r50 0.253\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 1.67s | valid loss 223438.55 | n100 0.279 | r20 0.206 | r50 0.257\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 1.65s | valid loss 220578.03 | n100 0.293 | r20 0.218 | r50 0.267\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 1.63s | valid loss 218455.54 | n100 0.301 | r20 0.226 | r50 0.275\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 1.71s | valid loss 215390.25 | n100 0.308 | r20 0.231 | r50 0.282\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 1.58s | valid loss 213192.98 | n100 0.314 | r20 0.235 | r50 0.288\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 1.55s | valid loss 211420.12 | n100 0.318 | r20 0.240 | r50 0.290\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 1.62s | valid loss 209936.40 | n100 0.323 | r20 0.241 | r50 0.294\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 1.79s | valid loss 209086.19 | n100 0.327 | r20 0.244 | r50 0.298\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 1.57s | valid loss 207423.29 | n100 0.332 | r20 0.252 | r50 0.303\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 1.67s | valid loss 207235.53 | n100 0.335 | r20 0.254 | r50 0.305\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 1.62s | valid loss 205082.08 | n100 0.339 | r20 0.256 | r50 0.309\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 1.63s | valid loss 205103.15 | n100 0.340 | r20 0.257 | r50 0.311\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 1.66s | valid loss 203284.01 | n100 0.345 | r20 0.261 | r50 0.316\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 1.74s | valid loss 202348.43 | n100 0.347 | r20 0.263 | r50 0.316\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 1.71s | valid loss 201565.87 | n100 0.349 | r20 0.266 | r50 0.320\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 1.58s | valid loss 200390.85 | n100 0.354 | r20 0.270 | r50 0.324\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 1.66s | valid loss 199629.22 | n100 0.356 | r20 0.271 | r50 0.324\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 1.73s | valid loss 199495.28 | n100 0.361 | r20 0.276 | r50 0.330\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 1.64s | valid loss 199266.15 | n100 0.362 | r20 0.274 | r50 0.330\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 1.57s | valid loss 197276.33 | n100 0.364 | r20 0.277 | r50 0.332\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 1.58s | valid loss 196141.80 | n100 0.366 | r20 0.277 | r50 0.334\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 1.49s | valid loss 195886.70 | n100 0.372 | r20 0.283 | r50 0.338\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 1.60s | valid loss 194509.86 | n100 0.372 | r20 0.283 | r50 0.338\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 1.61s | valid loss 194415.50 | n100 0.375 | r20 0.284 | r50 0.340\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 1.57s | valid loss 193393.41 | n100 0.379 | r20 0.288 | r50 0.346\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 1.67s | valid loss 192620.80 | n100 0.382 | r20 0.292 | r50 0.348\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 1.54s | valid loss 190974.59 | n100 0.384 | r20 0.294 | r50 0.352\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 1.69s | valid loss 190578.31 | n100 0.387 | r20 0.297 | r50 0.355\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 1.65s | valid loss 189690.88 | n100 0.388 | r20 0.298 | r50 0.356\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 1.72s | valid loss 188485.09 | n100 0.392 | r20 0.301 | r50 0.359\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 1.72s | valid loss 188613.91 | n100 0.395 | r20 0.303 | r50 0.361\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 1.64s | valid loss 187533.64 | n100 0.395 | r20 0.304 | r50 0.362\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 1.69s | valid loss 187454.25 | n100 0.400 | r20 0.308 | r50 0.366\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 1.70s | valid loss 185102.73 | n100 0.402 | r20 0.310 | r50 0.368\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 1.59s | valid loss 184462.29 | n100 0.404 | r20 0.312 | r50 0.370\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 1.61s | valid loss 183196.74 | n100 0.408 | r20 0.313 | r50 0.374\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 1.64s | valid loss 182096.70 | n100 0.410 | r20 0.316 | r50 0.375\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 1.78s | valid loss 182216.62 | n100 0.412 | r20 0.318 | r50 0.376\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 1.73s | valid loss 180448.68 | n100 0.415 | r20 0.321 | r50 0.379\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 1.73s | valid loss 179923.16 | n100 0.418 | r20 0.323 | r50 0.381\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time: 1.68s | valid loss 178872.23 | n100 0.420 | r20 0.325 | r50 0.385\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time: 1.64s | valid loss 178255.58 | n100 0.424 | r20 0.329 | r50 0.388\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time: 1.73s | valid loss 176128.51 | n100 0.426 | r20 0.331 | r50 0.388\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time: 1.66s | valid loss 175767.60 | n100 0.429 | r20 0.332 | r50 0.392\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time: 1.62s | valid loss 174218.63 | n100 0.432 | r20 0.336 | r50 0.396\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time: 1.58s | valid loss 174354.38 | n100 0.434 | r20 0.339 | r50 0.397\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time: 1.59s | valid loss 173144.85 | n100 0.437 | r20 0.341 | r50 0.401\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time: 1.67s | valid loss 171819.12 | n100 0.440 | r20 0.343 | r50 0.402\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time: 1.71s | valid loss 169900.47 | n100 0.441 | r20 0.344 | r50 0.403\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss 166396.17 | n100 0.44 | r20 0.34 | r50 0.41\n",
            "=========================================================================================\n",
            "best epoch: 59\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, args.epochs + 40):\n",
        "    epoch_start_time = time.time()\n",
        "    train(model, criterion, optimizer, is_VAE=True)\n",
        "    val_loss, n100, r20, r50 = evaluate(model, criterion, vad_data_tr, vad_data_te, is_VAE=True)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:4.2f}s | valid loss {:4.2f} | '\n",
        "            'n100 {:5.3f} | r20 {:5.3f} | r50 {:5.3f}'.format(\n",
        "                epoch, time.time() - epoch_start_time, val_loss,\n",
        "                n100, r20, r50))\n",
        "    print('-' * 89)\n",
        "\n",
        "    n_iter = epoch * len(range(0, N, args.batch_size))\n",
        "\n",
        "\n",
        "    # Save the model if the n100 is the best we've seen so far.\n",
        "    if n100 > best_n100:\n",
        "        with open(args.save, 'wb') as f:\n",
        "            torch.save(model, f)\n",
        "        best_n100 = n100\n",
        "        best_epoch = epoch\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(args.save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss, n100, r20, r50 = evaluate(model, criterion, test_data_tr, test_data_te, is_VAE=True)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:4.2f} | n100 {:4.2f} | r20 {:4.2f} | '\n",
        "        'r50 {:4.2f}'.format(test_loss, n100, r20, r50))\n",
        "print('=' * 89)\n",
        "print('best epoch:', best_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xBh0Mrcn6Af"
      },
      "source": [
        "## Required Package\n",
        "\n",
        "- numpy==1.23.5\n",
        "- pandas==1.5.3\n",
        "- torch==2.1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQjnnLZ-Jf5b"
      },
      "source": [
        "###**콘텐츠 라이선스**\n",
        "\n",
        "<font color='red'><b>**WARNING**</b></font> : **본 교육 콘텐츠의 지식재산권은 재단법인 네이버커넥트에 귀속됩니다. 본 콘텐츠를 어떠한 경로로든 외부로 유출 및 수정하는 행위를 엄격히 금합니다.** 다만, 비영리적 교육 및 연구활동에 한정되어 사용할 수 있으나 재단의 허락을 받아야 합니다. 이를 위반하는 경우, 관련 법률에 따라 책임을 질 수 있습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference\n",
        "\n",
        "전체 학습 -> inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_N = raw_data.user.nunique()\n",
        "idx_list = np.arange(total_N)\n",
        "\n",
        "id2show = {v:k for k,v in show2id.items()}\n",
        "id2profile = {v:k for k,v in profile2id.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 410,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'model_total.pt'"
            ]
          },
          "execution_count": 410,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "args.save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 340,
      "metadata": {},
      "outputs": [],
      "source": [
        "def inference(model, criterion, data, is_VAE=False):\n",
        "   \n",
        "# Load the best saved model.\n",
        "    with open(args.save, 'rb') as f:\n",
        "      model = torch.load(f)\n",
        "    \n",
        "    # turn on eval mode\n",
        "    model.eval()\n",
        "    global update_count\n",
        "    idxlist = list(range(data.shape[0]))\n",
        "    N = data.shape[0]\n",
        "    total_topk = []\n",
        "    result = pd.DataFrame()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for start_idx in range(0, N, args.batch_size):\n",
        "            end_idx = min(start_idx + args.batch_size, N)\n",
        "            batch = data[idxlist[start_idx:end_idx]]\n",
        "\n",
        "            # data_tensor = naive_sparse2tensor(data).to(device)\n",
        "            data_tensor = sparse2torch_sparse(batch).to(device)\n",
        "            if is_VAE :\n",
        "\n",
        "              if args.total_anneal_steps > 0:\n",
        "                  anneal = min(args.anneal_cap,\n",
        "                                1. * update_count / args.total_anneal_steps)\n",
        "              else:\n",
        "                  anneal = args.anneal_cap\n",
        "\n",
        "              recon_batch, mu, logvar = model(data_tensor)\n",
        "\n",
        "            else :\n",
        "              recon_batch = model(data_tensor)\n",
        "\n",
        "            # Exclude examples from training set\n",
        "            recon_batch[batch.nonzero()] = -1e9\n",
        "\n",
        "            topk = torch.topk(input=recon_batch, k=10)\n",
        "            batch_topk = list(topk.indices.reshape(-1).detach().cpu().numpy())\n",
        "          \n",
        "            total_topk.extend(batch_topk)\n",
        "\n",
        "\n",
        "    result['user'] = np.repeat(np.arange(31360),10)\n",
        "    result['item'] = total_topk\n",
        "\n",
        "    result['user'] = result['user'].apply(lambda x: id2profile[x])\n",
        "    result['item'] = result['item'].apply(lambda x: id2show[x])\n",
        "    \n",
        "    return result.sort_values('user').reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 375,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrain(model, criterion, optimizer, train_data=data_inf, is_VAE = False):\n",
        "    # Turn on training mode\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    start_time = time.time()\n",
        "    global update_count\n",
        "    N = train_data.shape[0]\n",
        "    idxlist = np.arange(N)\n",
        "\n",
        "    np.random.shuffle(idxlist)\n",
        "\n",
        "    for batch_idx, start_idx in enumerate(range(0, N, args.batch_size)):\n",
        "        end_idx = min(start_idx + args.batch_size, N)\n",
        "        batch = train_data[idxlist[start_idx:end_idx]] # 여기의 train_data: sparse interaction matrix 형태\n",
        "        data = naive_sparse2tensor(batch).to(device)\n",
        "        # data = sparse2torch_sparse(batch).to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if is_VAE:\n",
        "          if args.total_anneal_steps > 0:\n",
        "            anneal = min(args.anneal_cap,\n",
        "                            1. * update_count / args.total_anneal_steps)\n",
        "          else:\n",
        "              anneal = args.anneal_cap\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          recon_batch, mu, logvar = model(data)\n",
        "\n",
        "          loss = criterion(recon_batch, data, mu, logvar, anneal)\n",
        "        else:\n",
        "          recon_batch = model(data)\n",
        "          loss = criterion(recon_batch, data)\n",
        "\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "        update_count += 1\n",
        "\n",
        "        if batch_idx % args.log_interval == 0 and batch_idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:4d}/{:4d} batches | ms/batch {:4.2f} | '\n",
        "                    'loss {:4.2f}'.format(\n",
        "                        epoch, batch_idx, len(range(0, N, args.batch_size)),\n",
        "                        elapsed * 1000 / args.log_interval,\n",
        "                        train_loss / args.log_interval))\n",
        "\n",
        "\n",
        "            start_time = time.time()\n",
        "            train_loss = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 405,
      "metadata": {},
      "outputs": [],
      "source": [
        "p_dims = [200, 600, n_items]\n",
        "model = MultiVAE(p_dims).to(device)\n",
        "loader = DataLoader(args.data)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=args.wd)\n",
        "criterion = loss_function_vae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 406,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/ml/miniconda3/envs/movie/lib/python3.10/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 1.62s | valid loss 257006.02 | n100 0.261 | r20 0.191 | r50 0.236\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 1.56s | valid loss 241311.42 | n100 0.263 | r20 0.191 | r50 0.239\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 1.49s | valid loss 235747.03 | n100 0.262 | r20 0.190 | r50 0.240\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 1.51s | valid loss 233817.99 | n100 0.263 | r20 0.192 | r50 0.240\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 1.53s | valid loss 232515.37 | n100 0.263 | r20 0.191 | r50 0.239\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 1.59s | valid loss 230377.44 | n100 0.266 | r20 0.194 | r50 0.241\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 1.65s | valid loss 228247.84 | n100 0.270 | r20 0.199 | r50 0.246\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 1.48s | valid loss 226076.11 | n100 0.272 | r20 0.202 | r50 0.250\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 1.59s | valid loss 224181.16 | n100 0.277 | r20 0.203 | r50 0.253\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 1.55s | valid loss 222173.60 | n100 0.284 | r20 0.209 | r50 0.261\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 1.52s | valid loss 218874.01 | n100 0.299 | r20 0.225 | r50 0.275\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 1.67s | valid loss 214986.12 | n100 0.313 | r20 0.236 | r50 0.287\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 1.62s | valid loss 212032.61 | n100 0.322 | r20 0.242 | r50 0.294\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 1.58s | valid loss 209220.15 | n100 0.326 | r20 0.247 | r50 0.299\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 1.63s | valid loss 208625.19 | n100 0.331 | r20 0.249 | r50 0.303\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 1.54s | valid loss 206837.45 | n100 0.335 | r20 0.253 | r50 0.305\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 1.70s | valid loss 205307.43 | n100 0.338 | r20 0.255 | r50 0.309\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 1.73s | valid loss 203582.67 | n100 0.344 | r20 0.259 | r50 0.312\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 1.68s | valid loss 202094.72 | n100 0.346 | r20 0.260 | r50 0.316\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 1.64s | valid loss 201125.92 | n100 0.352 | r20 0.267 | r50 0.322\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 1.58s | valid loss 200763.41 | n100 0.356 | r20 0.269 | r50 0.324\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 1.64s | valid loss 198083.63 | n100 0.361 | r20 0.274 | r50 0.329\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 1.62s | valid loss 197762.70 | n100 0.364 | r20 0.275 | r50 0.334\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 1.64s | valid loss 195684.05 | n100 0.367 | r20 0.279 | r50 0.335\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 1.67s | valid loss 195286.76 | n100 0.369 | r20 0.280 | r50 0.338\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 1.57s | valid loss 194224.64 | n100 0.371 | r20 0.283 | r50 0.339\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 1.69s | valid loss 194096.60 | n100 0.374 | r20 0.285 | r50 0.341\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 1.64s | valid loss 193755.89 | n100 0.374 | r20 0.285 | r50 0.342\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 1.68s | valid loss 192048.51 | n100 0.378 | r20 0.288 | r50 0.345\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 1.65s | valid loss 191538.78 | n100 0.380 | r20 0.291 | r50 0.348\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 1.60s | valid loss 190422.60 | n100 0.384 | r20 0.293 | r50 0.351\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 1.69s | valid loss 189737.52 | n100 0.386 | r20 0.295 | r50 0.352\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 1.64s | valid loss 189581.29 | n100 0.388 | r20 0.297 | r50 0.355\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 1.69s | valid loss 188167.28 | n100 0.391 | r20 0.301 | r50 0.357\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 1.67s | valid loss 187711.42 | n100 0.395 | r20 0.302 | r50 0.360\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 1.64s | valid loss 185742.16 | n100 0.397 | r20 0.305 | r50 0.365\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 1.64s | valid loss 185476.23 | n100 0.402 | r20 0.308 | r50 0.368\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 1.60s | valid loss 183350.55 | n100 0.404 | r20 0.312 | r50 0.371\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 1.66s | valid loss 182391.61 | n100 0.407 | r20 0.312 | r50 0.374\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 1.58s | valid loss 182151.28 | n100 0.411 | r20 0.316 | r50 0.375\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 1.59s | valid loss 181296.33 | n100 0.414 | r20 0.320 | r50 0.379\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 1.65s | valid loss 178762.14 | n100 0.418 | r20 0.323 | r50 0.382\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 1.62s | valid loss 178244.50 | n100 0.421 | r20 0.326 | r50 0.387\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 1.73s | valid loss 177284.66 | n100 0.425 | r20 0.328 | r50 0.389\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 1.61s | valid loss 175915.60 | n100 0.428 | r20 0.334 | r50 0.391\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 1.68s | valid loss 175697.83 | n100 0.430 | r20 0.336 | r50 0.393\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 1.64s | valid loss 174141.16 | n100 0.435 | r20 0.339 | r50 0.395\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 1.64s | valid loss 172049.71 | n100 0.436 | r20 0.341 | r50 0.398\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 1.69s | valid loss 171542.51 | n100 0.439 | r20 0.343 | r50 0.400\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 1.66s | valid loss 170064.88 | n100 0.441 | r20 0.345 | r50 0.403\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time: 1.64s | valid loss 169622.47 | n100 0.443 | r20 0.346 | r50 0.404\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time: 1.71s | valid loss 169119.76 | n100 0.443 | r20 0.347 | r50 0.405\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time: 1.59s | valid loss 167051.82 | n100 0.447 | r20 0.349 | r50 0.408\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time: 1.68s | valid loss 166551.84 | n100 0.448 | r20 0.351 | r50 0.409\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time: 1.57s | valid loss 165562.39 | n100 0.448 | r20 0.352 | r50 0.411\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time: 1.72s | valid loss 165819.22 | n100 0.451 | r20 0.354 | r50 0.412\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time: 1.64s | valid loss 164835.88 | n100 0.452 | r20 0.356 | r50 0.412\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time: 1.60s | valid loss 162825.06 | n100 0.452 | r20 0.356 | r50 0.412\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time: 1.71s | valid loss 161969.43 | n100 0.453 | r20 0.356 | r50 0.413\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss 158710.63 | n100 0.45 | r20 0.35 | r50 0.42\n",
            "=========================================================================================\n",
            "best epoch: 59\n"
          ]
        }
      ],
      "source": [
        "train_data = loader.load_data('train')\n",
        "vad_data_tr, vad_data_te = loader.load_data('validation')\n",
        "test_data_tr, test_data_te = loader._load_tr_te_data('test')\n",
        "data_inf = loader.load_data('inference')\n",
        "\n",
        "N = train_data.shape[0]\n",
        "idx_list = np.arange(N)\n",
        "\n",
        "best_n100 = -np.inf\n",
        "for epoch in range(1, args.epochs + 40):\n",
        "    epoch_start_time = time.time()\n",
        "    # retrain(model, criterion, optimizer, data_inf, is_VAE=True)\n",
        "    train(model, criterion, optimizer, is_VAE=True)\n",
        "    val_loss, n100, r20, r50 = evaluate(model, criterion, vad_data_tr, vad_data_te, is_VAE=True)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:4.2f}s | valid loss {:4.2f} | '\n",
        "            'n100 {:5.3f} | r20 {:5.3f} | r50 {:5.3f}'.format(\n",
        "                epoch, time.time() - epoch_start_time, val_loss,\n",
        "                n100, r20, r50))\n",
        "    print('-' * 89)\n",
        "\n",
        "    n_iter = epoch * len(range(0, N, args.batch_size))\n",
        "\n",
        "\n",
        "    # Save the model if the n100 is the best we've seen so far.\n",
        "    if n100 > best_n100:\n",
        "        with open('model_total.pt', 'wb') as f:\n",
        "            torch.save(model, f)\n",
        "        best_n100 = n100\n",
        "        best_epoch=epoch\n",
        "\n",
        "\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(args.save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss, n100, r20, r50 = evaluate(model, criterion, test_data_tr, test_data_te, is_VAE=True)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:4.2f} | n100 {:4.2f} | r20 {:4.2f} | '\n",
        "        'r50 {:4.2f}'.format(test_loss, n100, r20, r50))\n",
        "print('=' * 89)\n",
        "print('best epoch:', best_epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 407,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/ml/miniconda3/envs/movie/lib/python3.10/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        }
      ],
      "source": [
        "args.save='model_total.pt'\n",
        "result = inference(model, criterion, data_inf, is_VAE=True)\n",
        "result.to_csv('multi-vae.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 408,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user</th>\n",
              "      <th>item</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11</td>\n",
              "      <td>2329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11</td>\n",
              "      <td>72998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11</td>\n",
              "      <td>32587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11</td>\n",
              "      <td>34405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>4370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313595</th>\n",
              "      <td>138493</td>\n",
              "      <td>8961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313596</th>\n",
              "      <td>138493</td>\n",
              "      <td>1270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313597</th>\n",
              "      <td>138493</td>\n",
              "      <td>2012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313598</th>\n",
              "      <td>138493</td>\n",
              "      <td>32587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313599</th>\n",
              "      <td>138493</td>\n",
              "      <td>4995</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>313600 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          user   item\n",
              "0           11   2329\n",
              "1           11  72998\n",
              "2           11  32587\n",
              "3           11  34405\n",
              "4           11   4370\n",
              "...        ...    ...\n",
              "313595  138493   8961\n",
              "313596  138493   1270\n",
              "313597  138493   2012\n",
              "313598  138493  32587\n",
              "313599  138493   4995\n",
              "\n",
              "[313600 rows x 2 columns]"
            ]
          },
          "execution_count": 408,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 325,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user</th>\n",
              "      <th>item</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11</td>\n",
              "      <td>5218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11</td>\n",
              "      <td>8861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11</td>\n",
              "      <td>3986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11</td>\n",
              "      <td>2054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>4370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313595</th>\n",
              "      <td>138493</td>\n",
              "      <td>4370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313596</th>\n",
              "      <td>138493</td>\n",
              "      <td>589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313597</th>\n",
              "      <td>138493</td>\n",
              "      <td>2011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313598</th>\n",
              "      <td>138493</td>\n",
              "      <td>33615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313599</th>\n",
              "      <td>138493</td>\n",
              "      <td>593</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>313600 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          user   item\n",
              "0           11   5218\n",
              "1           11   8861\n",
              "2           11   3986\n",
              "3           11   2054\n",
              "4           11   4370\n",
              "...        ...    ...\n",
              "313595  138493   4370\n",
              "313596  138493    589\n",
              "313597  138493   2011\n",
              "313598  138493  33615\n",
              "313599  138493    593\n",
              "\n",
              "[313600 rows x 2 columns]"
            ]
          },
          "execution_count": 325,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('json_id/id2show.json', 'r') as json_file:\n",
        "    id2show = json.load(json_file)\n",
        "id2show = json.loads(id2show)\n",
        "\n",
        "with open('json_id/id2profile.json', 'r') as json_file:\n",
        "    id2profile = json.load(json_file)\n",
        "id2profile = json.loads(id2profile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'0': 4643,\n",
              " '1': 170,\n",
              " '2': 531,\n",
              " '3': 616,\n",
              " '4': 2140,\n",
              " '5': 2722,\n",
              " '6': 2313,\n",
              " '7': 2688,\n",
              " '8': 2428,\n",
              " '9': 3113,\n",
              " '10': 1591,\n",
              " '11': 2600,\n",
              " '12': 8169,\n",
              " '13': 2572,\n",
              " '14': 58293,\n",
              " '15': 7541,\n",
              " '16': 1367,\n",
              " '17': 32,\n",
              " '18': 4792,\n",
              " '19': 7444,\n",
              " '20': 53953,\n",
              " '21': 56949,\n",
              " '22': 6502,\n",
              " '23': 53000,\n",
              " '24': 51662,\n",
              " '25': 5151,\n",
              " '26': 35836,\n",
              " '27': 7293,\n",
              " '28': 33585,\n",
              " '29': 8810,\n",
              " '30': 56801,\n",
              " '31': 5377,\n",
              " '32': 344,\n",
              " '33': 19,\n",
              " '34': 410,\n",
              " '35': 2124,\n",
              " '36': 828,\n",
              " '37': 1274,\n",
              " '38': 8977,\n",
              " '39': 1032,\n",
              " '40': 1214,\n",
              " '41': 1200,\n",
              " '42': 1320,\n",
              " '43': 3897,\n",
              " '44': 7173,\n",
              " '45': 1225,\n",
              " '46': 2858,\n",
              " '47': 59418,\n",
              " '48': 45361,\n",
              " '49': 2706,\n",
              " '50': 1321,\n",
              " '51': 2793,\n",
              " '52': 33085,\n",
              " '53': 4235,\n",
              " '54': 3892,\n",
              " '55': 4340,\n",
              " '56': 27660,\n",
              " '57': 43556,\n",
              " '58': 47124,\n",
              " '59': 2294,\n",
              " '60': 48304,\n",
              " '61': 150,\n",
              " '62': 31184,\n",
              " '63': 34338,\n",
              " '64': 1917,\n",
              " '65': 50162,\n",
              " '66': 2827,\n",
              " '67': 27368,\n",
              " '68': 4366,\n",
              " '69': 2153,\n",
              " '70': 30812,\n",
              " '71': 3525,\n",
              " '72': 1270,\n",
              " '73': 2011,\n",
              " '74': 2012,\n",
              " '75': 8973,\n",
              " '76': 1255,\n",
              " '77': 2018,\n",
              " '78': 541,\n",
              " '79': 4878,\n",
              " '80': 7361,\n",
              " '81': 31658,\n",
              " '82': 2571,\n",
              " '83': 7099,\n",
              " '84': 260,\n",
              " '85': 1196,\n",
              " '86': 60069,\n",
              " '87': 160,\n",
              " '88': 1882,\n",
              " '89': 60037,\n",
              " '90': 880,\n",
              " '91': 36509,\n",
              " '92': 405,\n",
              " '93': 3826,\n",
              " '94': 4133,\n",
              " '95': 673,\n",
              " '96': 6541,\n",
              " '97': 611,\n",
              " '98': 172,\n",
              " '99': 4638,\n",
              " '100': 5171,\n",
              " '101': 208,\n",
              " '102': 4887,\n",
              " '103': 5459,\n",
              " '104': 60760,\n",
              " '105': 8361,\n",
              " '106': 60514,\n",
              " '107': 1544,\n",
              " '108': 1876,\n",
              " '109': 442,\n",
              " '110': 32213,\n",
              " '111': 5219,\n",
              " '112': 1690,\n",
              " '113': 2717,\n",
              " '114': 27608,\n",
              " '115': 52722,\n",
              " '116': 780,\n",
              " '117': 6934,\n",
              " '118': 52287,\n",
              " '119': 3745,\n",
              " '120': 45499,\n",
              " '121': 37830,\n",
              " '122': 60040,\n",
              " '123': 34319,\n",
              " '124': 8644,\n",
              " '125': 6365,\n",
              " '126': 34048,\n",
              " '127': 316,\n",
              " '128': 3300,\n",
              " '129': 7022,\n",
              " '130': 7254,\n",
              " '131': 57368,\n",
              " '132': 1584,\n",
              " '133': 2232,\n",
              " '134': 1748,\n",
              " '135': 1253,\n",
              " '136': 49278,\n",
              " '137': 42718,\n",
              " '138': 1097,\n",
              " '139': 7481,\n",
              " '140': 5903,\n",
              " '141': 1653,\n",
              " '142': 741,\n",
              " '143': 27728,\n",
              " '144': 2716,\n",
              " '145': 52281,\n",
              " '146': 57640,\n",
              " '147': 2761,\n",
              " '148': 59315,\n",
              " '149': 480,\n",
              " '150': 3702,\n",
              " '151': 1580,\n",
              " '152': 5445,\n",
              " '153': 52885,\n",
              " '154': 3527,\n",
              " '155': 44849,\n",
              " '156': 27904,\n",
              " '157': 5349,\n",
              " '158': 8636,\n",
              " '159': 589,\n",
              " '160': 1240,\n",
              " '161': 2916,\n",
              " '162': 3793,\n",
              " '163': 6333,\n",
              " '164': 3054,\n",
              " '165': 546,\n",
              " '166': 256,\n",
              " '167': 173,\n",
              " '168': 2642,\n",
              " '169': 7846,\n",
              " '170': 8870,\n",
              " '171': 54771,\n",
              " '172': 7845,\n",
              " '173': 34150,\n",
              " '174': 51412,\n",
              " '175': 3697,\n",
              " '176': 6534,\n",
              " '177': 7163,\n",
              " '178': 5463,\n",
              " '179': 3438,\n",
              " '180': 3269,\n",
              " '181': 8371,\n",
              " '182': 5046,\n",
              " '183': 6537,\n",
              " '184': 4446,\n",
              " '185': 8865,\n",
              " '186': 5378,\n",
              " '187': 32031,\n",
              " '188': 3704,\n",
              " '189': 4533,\n",
              " '190': 6659,\n",
              " '191': 2134,\n",
              " '192': 1676,\n",
              " '193': 2105,\n",
              " '194': 5502,\n",
              " '195': 3033,\n",
              " '196': 592,\n",
              " '197': 56174,\n",
              " '198': 318,\n",
              " '199': 60832,\n",
              " '200': 4993,\n",
              " '201': 3114,\n",
              " '202': 1136,\n",
              " '203': 1073,\n",
              " '204': 648,\n",
              " '205': 110,\n",
              " '206': 8622,\n",
              " '207': 6874,\n",
              " '208': 6863,\n",
              " '209': 6539,\n",
              " '210': 6377,\n",
              " '211': 5816,\n",
              " '212': 5669,\n",
              " '213': 5618,\n",
              " '214': 5299,\n",
              " '215': 3863,\n",
              " '216': 3752,\n",
              " '217': 2355,\n",
              " '218': 2291,\n",
              " '219': 1339,\n",
              " '220': 1029,\n",
              " '221': 1028,\n",
              " '222': 784,\n",
              " '223': 610,\n",
              " '224': 551,\n",
              " '225': 158,\n",
              " '226': 54001,\n",
              " '227': 50872,\n",
              " '228': 48394,\n",
              " '229': 45722,\n",
              " '230': 33679,\n",
              " '231': 31878,\n",
              " '232': 30793,\n",
              " '233': 6953,\n",
              " '234': 6503,\n",
              " '235': 6281,\n",
              " '236': 5782,\n",
              " '237': 5400,\n",
              " '238': 5266,\n",
              " '239': 4388,\n",
              " '240': 4299,\n",
              " '241': 4054,\n",
              " '242': 3988,\n",
              " '243': 3980,\n",
              " '244': 3969,\n",
              " '245': 3247,\n",
              " '246': 2840,\n",
              " '247': 2720,\n",
              " '248': 2004,\n",
              " '249': 761,\n",
              " '250': 56757,\n",
              " '251': 41571,\n",
              " '252': 36401,\n",
              " '253': 8965,\n",
              " '254': 8907,\n",
              " '255': 8640,\n",
              " '256': 7439,\n",
              " '257': 6996,\n",
              " '258': 5283,\n",
              " '259': 4974,\n",
              " '260': 2959,\n",
              " '261': 2240,\n",
              " '262': 2042,\n",
              " '263': 2028,\n",
              " '264': 296,\n",
              " '265': 356,\n",
              " '266': 593,\n",
              " '267': 1,\n",
              " '268': 1210,\n",
              " '269': 608,\n",
              " '270': 377,\n",
              " '271': 1198,\n",
              " '272': 165,\n",
              " '273': 595,\n",
              " '274': 153,\n",
              " '275': 364,\n",
              " '276': 597,\n",
              " '277': 231,\n",
              " '278': 500,\n",
              " '279': 367,\n",
              " '280': 1721,\n",
              " '281': 587,\n",
              " '282': 1197,\n",
              " '283': 2628,\n",
              " '284': 253,\n",
              " '285': 1291,\n",
              " '286': 3578,\n",
              " '287': 1036,\n",
              " '288': 185,\n",
              " '289': 4306,\n",
              " '290': 586,\n",
              " '291': 39,\n",
              " '292': 5952,\n",
              " '293': 2396,\n",
              " '294': 1961,\n",
              " '295': 1527,\n",
              " '296': 1387,\n",
              " '297': 8368,\n",
              " '298': 6378,\n",
              " '299': 6016,\n",
              " '300': 4896,\n",
              " '301': 3000,\n",
              " '302': 2810,\n",
              " '303': 3159,\n",
              " '304': 7458,\n",
              " '305': 5152,\n",
              " '306': 33493,\n",
              " '307': 41566,\n",
              " '308': 4973,\n",
              " '309': 33794,\n",
              " '310': 4995,\n",
              " '311': 33660,\n",
              " '312': 33166,\n",
              " '313': 58559,\n",
              " '314': 62265,\n",
              " '315': 27441,\n",
              " '316': 67295,\n",
              " '317': 653,\n",
              " '318': 55999,\n",
              " '319': 63859,\n",
              " '320': 3753,\n",
              " '321': 8972,\n",
              " '322': 53125,\n",
              " '323': 3083,\n",
              " '324': 5225,\n",
              " '325': 1046,\n",
              " '326': 64034,\n",
              " '327': 53996,\n",
              " '328': 43932,\n",
              " '329': 26887,\n",
              " '330': 66171,\n",
              " '331': 68319,\n",
              " '332': 51698,\n",
              " '333': 5444,\n",
              " '334': 6731,\n",
              " '335': 2455,\n",
              " '336': 56145,\n",
              " '337': 968,\n",
              " '338': 54995,\n",
              " '339': 52328,\n",
              " '340': 3994,\n",
              " '341': 44191,\n",
              " '342': 6979,\n",
              " '343': 60684,\n",
              " '344': 6250,\n",
              " '345': 44731,\n",
              " '346': 64497,\n",
              " '347': 3864,\n",
              " '348': 6294,\n",
              " '349': 64997,\n",
              " '350': 67197,\n",
              " '351': 69526,\n",
              " '352': 5882,\n",
              " '353': 31660,\n",
              " '354': 1391,\n",
              " '355': 4571,\n",
              " '356': 4492,\n",
              " '357': 2985,\n",
              " '358': 380,\n",
              " '359': 527,\n",
              " '360': 588,\n",
              " '361': 736,\n",
              " '362': 2762,\n",
              " '363': 41569,\n",
              " '364': 72641,\n",
              " '365': 48774,\n",
              " '366': 48516,\n",
              " '367': 71282,\n",
              " '368': 64614,\n",
              " '369': 34542,\n",
              " '370': 65514,\n",
              " '371': 48738,\n",
              " '372': 6291,\n",
              " '373': 46578,\n",
              " '374': 7153,\n",
              " '375': 4226,\n",
              " '376': 1952,\n",
              " '377': 1283,\n",
              " '378': 3507,\n",
              " '379': 4280,\n",
              " '380': 51084,\n",
              " '381': 42197,\n",
              " '382': 52375,\n",
              " '383': 44555,\n",
              " '384': 31410,\n",
              " '385': 43376,\n",
              " '386': 8645,\n",
              " '387': 27803,\n",
              " '388': 7767,\n",
              " '389': 44204,\n",
              " '390': 7323,\n",
              " '391': 44694,\n",
              " '392': 5878,\n",
              " '393': 5608,\n",
              " '394': 5477,\n",
              " '395': 41527,\n",
              " '396': 5613,\n",
              " '397': 30749,\n",
              " '398': 55052,\n",
              " '399': 1305,\n",
              " '400': 1211,\n",
              " '401': 1209,\n",
              " '402': 7034,\n",
              " '403': 4967,\n",
              " '404': 1252,\n",
              " '405': 1247,\n",
              " '406': 7234,\n",
              " '407': 1201,\n",
              " '408': 1206,\n",
              " '409': 45950,\n",
              " '410': 56788,\n",
              " '411': 55820,\n",
              " '412': 6197,\n",
              " '413': 3426,\n",
              " '414': 61075,\n",
              " '415': 2289,\n",
              " '416': 59118,\n",
              " '417': 7456,\n",
              " '418': 1641,\n",
              " '419': 6942,\n",
              " '420': 57792,\n",
              " '421': 1230,\n",
              " '422': 3812,\n",
              " '423': 3967,\n",
              " '424': 2997,\n",
              " '425': 6711,\n",
              " '426': 1221,\n",
              " '427': 858,\n",
              " '428': 64701,\n",
              " '429': 51174,\n",
              " '430': 59018,\n",
              " '431': 4428,\n",
              " '432': 69122,\n",
              " '433': 1784,\n",
              " '434': 8254,\n",
              " '435': 63062,\n",
              " '436': 186,\n",
              " '437': 60482,\n",
              " '438': 71033,\n",
              " '439': 76093,\n",
              " '440': 8961,\n",
              " '441': 87232,\n",
              " '442': 89745,\n",
              " '443': 112852,\n",
              " '444': 91500,\n",
              " '445': 98809,\n",
              " '446': 79091,\n",
              " '447': 86332,\n",
              " '448': 73321,\n",
              " '449': 82461,\n",
              " '450': 81564,\n",
              " '451': 79695,\n",
              " '452': 78469,\n",
              " '453': 86880,\n",
              " '454': 84944,\n",
              " '455': 78105,\n",
              " '456': 3671,\n",
              " '457': 5218,\n",
              " '458': 47610,\n",
              " '459': 73017,\n",
              " '460': 4016,\n",
              " '461': 7373,\n",
              " '462': 59369,\n",
              " '463': 5254,\n",
              " '464': 4343,\n",
              " '465': 91529,\n",
              " '466': 54259,\n",
              " '467': 6754,\n",
              " '468': 77561,\n",
              " '469': 88125,\n",
              " '470': 4232,\n",
              " '471': 81834,\n",
              " '472': 33615,\n",
              " '473': 60074,\n",
              " '474': 5574,\n",
              " '475': 5785,\n",
              " '476': 5110,\n",
              " '477': 44022,\n",
              " '478': 5267,\n",
              " '479': 6297,\n",
              " '480': 4886,\n",
              " '481': 70286,\n",
              " '482': 40815,\n",
              " '483': 72998,\n",
              " '484': 3916,\n",
              " '485': 54997,\n",
              " '486': 59784,\n",
              " '487': 69844,\n",
              " '488': 4700,\n",
              " '489': 46972,\n",
              " '490': 6156,\n",
              " '491': 88744,\n",
              " '492': 4386,\n",
              " '493': 118696,\n",
              " '494': 6218,\n",
              " '495': 38038,\n",
              " '496': 45517,\n",
              " '497': 8985,\n",
              " '498': 45431,\n",
              " '499': 3483,\n",
              " '500': 47200,\n",
              " '501': 42738,\n",
              " '502': 56171,\n",
              " '503': 102125,\n",
              " '504': 106487,\n",
              " '505': 51077,\n",
              " '506': 49274,\n",
              " '507': 7324,\n",
              " '508': 59501,\n",
              " '509': 106489,\n",
              " '510': 36519,\n",
              " '511': 57528,\n",
              " '512': 5504,\n",
              " '513': 49649,\n",
              " '514': 48322,\n",
              " '515': 4990,\n",
              " '516': 40339,\n",
              " '517': 6889,\n",
              " '518': 5804,\n",
              " '519': 76175,\n",
              " '520': 34332,\n",
              " '521': 8808,\n",
              " '522': 50798,\n",
              " '523': 87222,\n",
              " '524': 65682,\n",
              " '525': 61160,\n",
              " '526': 83613,\n",
              " '527': 87430,\n",
              " '528': 68205,\n",
              " '529': 85510,\n",
              " '530': 5389,\n",
              " '531': 6566,\n",
              " '532': 3751,\n",
              " '533': 6385,\n",
              " '534': 82459,\n",
              " '535': 37386,\n",
              " '536': 53121,\n",
              " '537': 88140,\n",
              " '538': 6753,\n",
              " '539': 53464,\n",
              " '540': 58025,\n",
              " '541': 56775,\n",
              " '542': 7317,\n",
              " '543': 5621,\n",
              " '544': 2501,\n",
              " '545': 1223,\n",
              " '546': 1019,\n",
              " '547': 3396,\n",
              " '548': 107,\n",
              " '549': 60,\n",
              " '550': 2300,\n",
              " '551': 2231,\n",
              " '552': 5902,\n",
              " '553': 2023,\n",
              " '554': 7438,\n",
              " '555': 27773,\n",
              " '556': 3556,\n",
              " '557': 3949,\n",
              " '558': 40819,\n",
              " '559': 50,\n",
              " '560': 47,\n",
              " '561': 1617,\n",
              " '562': 1265,\n",
              " '563': 6003,\n",
              " '564': 4954,\n",
              " '565': 2692,\n",
              " '566': 4011,\n",
              " '567': 2360,\n",
              " '568': 2542,\n",
              " '569': 44665,\n",
              " '570': 2329,\n",
              " '571': 293,\n",
              " '572': 778,\n",
              " '573': 1089,\n",
              " '574': 30820,\n",
              " '575': 111,\n",
              " '576': 41863,\n",
              " '577': 55995,\n",
              " '578': 55069,\n",
              " '579': 55269,\n",
              " '580': 55247,\n",
              " '581': 27005,\n",
              " '582': 1222,\n",
              " '583': 56367,\n",
              " '584': 55442,\n",
              " '585': 56782,\n",
              " '586': 54190,\n",
              " '587': 6773,\n",
              " '588': 48082,\n",
              " '589': 8983,\n",
              " '590': 54286,\n",
              " '591': 32587,\n",
              " '592': 7090,\n",
              " '593': 27592,\n",
              " '594': 38061,\n",
              " '595': 49272,\n",
              " '596': 36529,\n",
              " '597': 5418,\n",
              " '598': 8798,\n",
              " '599': 53519,\n",
              " '600': 7143,\n",
              " '601': 8665,\n",
              " '602': 53972,\n",
              " '603': 6618,\n",
              " '604': 7235,\n",
              " '605': 54736,\n",
              " '606': 49651,\n",
              " '607': 8968,\n",
              " '608': 31696,\n",
              " '609': 39435,\n",
              " '610': 47044,\n",
              " '611': 8984,\n",
              " '612': 55232,\n",
              " '613': 6595,\n",
              " '614': 49530,\n",
              " '615': 30810,\n",
              " '616': 27801,\n",
              " '617': 50005,\n",
              " '618': 33903,\n",
              " '619': 40870,\n",
              " '620': 55280,\n",
              " '621': 34437,\n",
              " '622': 4979,\n",
              " '623': 4381,\n",
              " '624': 5577,\n",
              " '625': 52975,\n",
              " '626': 36537,\n",
              " '627': 37729,\n",
              " '628': 54272,\n",
              " '629': 4014,\n",
              " '630': 3535,\n",
              " '631': 53322,\n",
              " '632': 8360,\n",
              " '633': 5991,\n",
              " '634': 33004,\n",
              " '635': 8376,\n",
              " '636': 56587,\n",
              " '637': 27797,\n",
              " '638': 5014,\n",
              " '639': 261,\n",
              " '640': 3082,\n",
              " '641': 1962,\n",
              " '642': 2881,\n",
              " '643': 2359,\n",
              " '644': 830,\n",
              " '645': 1680,\n",
              " '646': 5013,\n",
              " '647': 5380,\n",
              " '648': 838,\n",
              " '649': 1704,\n",
              " '650': 28,\n",
              " '651': 2918,\n",
              " '652': 232,\n",
              " '653': 1278,\n",
              " '654': 8970,\n",
              " '655': 62293,\n",
              " '656': 3079,\n",
              " '657': 8943,\n",
              " '658': 4246,\n",
              " '659': 7155,\n",
              " '660': 613,\n",
              " '661': 4308,\n",
              " '662': 1923,\n",
              " '663': 372,\n",
              " '664': 2145,\n",
              " '665': 1635,\n",
              " '666': 1354,\n",
              " '667': 6104,\n",
              " '668': 27266,\n",
              " '669': 910,\n",
              " '670': 1282,\n",
              " '671': 3148,\n",
              " '672': 308,\n",
              " '673': 2976,\n",
              " '674': 7365,\n",
              " '675': 1884,\n",
              " '676': 48043,\n",
              " '677': 1279,\n",
              " '678': 8638,\n",
              " '679': 7147,\n",
              " '680': 27721,\n",
              " '681': 30707,\n",
              " '682': 2843,\n",
              " '683': 1259,\n",
              " '684': 1246,\n",
              " '685': 590,\n",
              " '686': 539,\n",
              " '687': 357,\n",
              " '688': 1193,\n",
              " '689': 912,\n",
              " '690': 3996,\n",
              " '691': 924,\n",
              " '692': 1208,\n",
              " '693': 2019,\n",
              " '694': 2174,\n",
              " '695': 1080,\n",
              " '696': 1183,\n",
              " '697': 923,\n",
              " '698': 497,\n",
              " '699': 34,\n",
              " '700': 36,\n",
              " '701': 6,\n",
              " '702': 300,\n",
              " '703': 223,\n",
              " '704': 11,\n",
              " '705': 1393,\n",
              " '706': 440,\n",
              " '707': 1213,\n",
              " '708': 474,\n",
              " '709': 2683,\n",
              " '710': 508,\n",
              " '711': 509,\n",
              " '712': 1517,\n",
              " '713': 750,\n",
              " '714': 1101,\n",
              " '715': 266,\n",
              " '716': 235,\n",
              " '717': 337,\n",
              " '718': 2797,\n",
              " '719': 317,\n",
              " '720': 2791,\n",
              " '721': 1682,\n",
              " '722': 553,\n",
              " '723': 832,\n",
              " '724': 1485,\n",
              " '725': 1394,\n",
              " '726': 2000,\n",
              " '727': 2640,\n",
              " '728': 555,\n",
              " '729': 1288,\n",
              " '730': 151,\n",
              " '731': 2406,\n",
              " '732': 3408,\n",
              " '733': 1358,\n",
              " '734': 5060,\n",
              " '735': 1094,\n",
              " '736': 265,\n",
              " '737': 1639,\n",
              " '738': 2302,\n",
              " '739': 802,\n",
              " '740': 1234,\n",
              " '741': 1396,\n",
              " '742': 1608,\n",
              " '743': 1625,\n",
              " '744': 1266,\n",
              " '745': 953,\n",
              " '746': 342,\n",
              " '747': 552,\n",
              " '748': 1148,\n",
              " '749': 1207,\n",
              " '750': 3175,\n",
              " '751': 805,\n",
              " '752': 2321,\n",
              " '753': 1250,\n",
              " '754': 246,\n",
              " '755': 852,\n",
              " '756': 1408,\n",
              " '757': 2804,\n",
              " '758': 1199,\n",
              " '759': 1242,\n",
              " '760': 1302,\n",
              " '761': 3253,\n",
              " '762': 1909,\n",
              " '763': 45,\n",
              " '764': 785,\n",
              " '765': 2470,\n",
              " '766': 1276,\n",
              " '767': 1084,\n",
              " '768': 969,\n",
              " '769': 1285,\n",
              " '770': 1275,\n",
              " '771': 661,\n",
              " '772': 2770,\n",
              " '773': 1747,\n",
              " '774': 2407,\n",
              " '775': 1088,\n",
              " '776': 348,\n",
              " '777': 224,\n",
              " '778': 1263,\n",
              " '779': 1729,\n",
              " '780': 1262,\n",
              " '781': 3072,\n",
              " '782': 3255,\n",
              " '783': 800,\n",
              " '784': 2248,\n",
              " '785': 1172,\n",
              " '786': 2278,\n",
              " '787': 1357,\n",
              " '788': 2161,\n",
              " '789': 2193,\n",
              " '790': 468,\n",
              " '791': 1203,\n",
              " '792': 1228,\n",
              " '793': 1231,\n",
              " '794': 2150,\n",
              " '795': 3006,\n",
              " '796': 247,\n",
              " '797': 2968,\n",
              " '798': 3256,\n",
              " '799': 431,\n",
              " '800': 2108,\n",
              " '801': 1479,\n",
              " '802': 2021,\n",
              " '803': 1272,\n",
              " '804': 2746,\n",
              " '805': 4370,\n",
              " '806': 455,\n",
              " '807': 1299,\n",
              " '808': 2020,\n",
              " '809': 1244,\n",
              " '810': 3552,\n",
              " '811': 3108,\n",
              " '812': 2076,\n",
              " '813': 1921,\n",
              " '814': 2352,\n",
              " '815': 2908,\n",
              " '816': 5010,\n",
              " '817': 194,\n",
              " '818': 4720,\n",
              " '819': 3361,\n",
              " '820': 2243,\n",
              " '821': 1296,\n",
              " '822': 3107,\n",
              " '823': 162,\n",
              " '824': 1093,\n",
              " '825': 2420,\n",
              " '826': 1960,\n",
              " '827': 3386,\n",
              " '828': 2944,\n",
              " '829': 1292,\n",
              " '830': 3317,\n",
              " '831': 1965,\n",
              " '832': 1957,\n",
              " '833': 4447,\n",
              " '834': 994,\n",
              " '835': 5679,\n",
              " '836': 1586,\n",
              " '837': 5464,\n",
              " '838': 4848,\n",
              " '839': 3252,\n",
              " '840': 2369,\n",
              " '841': 1095,\n",
              " '842': 1175,\n",
              " '843': 1104,\n",
              " '844': 4002,\n",
              " '845': 1020,\n",
              " '846': 1449,\n",
              " '847': 4262,\n",
              " '848': 361,\n",
              " '849': 382,\n",
              " '850': 3105,\n",
              " '851': 2686,\n",
              " '852': 1441,\n",
              " '853': 4975,\n",
              " '854': 2616,\n",
              " '855': 3424,\n",
              " '856': 1958,\n",
              " '857': 4223,\n",
              " '858': 3100,\n",
              " '859': 2739,\n",
              " '860': 5481,\n",
              " '861': 1945,\n",
              " '862': 1124,\n",
              " '863': 1711,\n",
              " '864': 6807,\n",
              " '865': 5388,\n",
              " '866': 314,\n",
              " '867': 1959,\n",
              " '868': 2245,\n",
              " '869': 954,\n",
              " '870': 1678,\n",
              " '871': 2384,\n",
              " '872': 2116,\n",
              " '873': 1956,\n",
              " '874': 4734,\n",
              " '875': 6870,\n",
              " '876': 3098,\n",
              " '877': 3893,\n",
              " '878': 1693,\n",
              " '879': 1081,\n",
              " '880': 428,\n",
              " '881': 3168,\n",
              " '882': 1217,\n",
              " '883': 3504,\n",
              " '884': 3360,\n",
              " '885': 3037,\n",
              " '886': 3699,\n",
              " '887': 5956,\n",
              " '888': 2065,\n",
              " '889': 6373,\n",
              " '890': 233,\n",
              " '891': 3536,\n",
              " '892': 2803,\n",
              " '893': 2469,\n",
              " '894': 2917,\n",
              " '895': 5673,\n",
              " '896': 8784,\n",
              " '897': 2139,\n",
              " '898': 8464,\n",
              " '899': 1459,\n",
              " '900': 517,\n",
              " '901': 3019,\n",
              " '902': 1096,\n",
              " '903': 2912,\n",
              " '904': 8949,\n",
              " '905': 1303,\n",
              " '906': 4007,\n",
              " '907': 6565,\n",
              " '908': 3763,\n",
              " '909': 89,\n",
              " '910': 1238,\n",
              " '911': 4008,\n",
              " '912': 448,\n",
              " '913': 3468,\n",
              " '914': 950,\n",
              " '915': 322,\n",
              " '916': 2474,\n",
              " '917': 1237,\n",
              " '918': 720,\n",
              " '919': 1967,\n",
              " '920': 441,\n",
              " '921': 2085,\n",
              " '922': 71462,\n",
              " '923': 7256,\n",
              " '924': 745,\n",
              " '925': 79132,\n",
              " '926': 2324,\n",
              " '927': 33358,\n",
              " '928': 26760,\n",
              " '929': 4312,\n",
              " '930': 50912,\n",
              " '931': 157,\n",
              " '932': 36517,\n",
              " '933': 2009,\n",
              " '934': 2080,\n",
              " '935': 2144,\n",
              " '936': 1916,\n",
              " '937': 53894,\n",
              " '938': 55282,\n",
              " '939': 407,\n",
              " '940': 3261,\n",
              " '941': 2395,\n",
              " '942': 50442,\n",
              " '943': 7451,\n",
              " '944': 39414,\n",
              " '945': 52973,\n",
              " '946': 48385,\n",
              " '947': 4034,\n",
              " '948': 51255,\n",
              " '949': 40732,\n",
              " '950': 39427,\n",
              " '951': 2026,\n",
              " '952': 4239,\n",
              " '953': 46976,\n",
              " '954': 4367,\n",
              " '955': 1895,\n",
              " '956': 3911,\n",
              " '957': 54503,\n",
              " '958': 27879,\n",
              " '959': 47423,\n",
              " '960': 1732,\n",
              " '961': 6710,\n",
              " '962': 4027,\n",
              " '963': 4448,\n",
              " '964': 40583,\n",
              " '965': 141,\n",
              " '966': 33136,\n",
              " '967': 147,\n",
              " '968': 6223,\n",
              " '969': 8910,\n",
              " '970': 2502,\n",
              " '971': 3481,\n",
              " '972': 34162,\n",
              " '973': 8874,\n",
              " '974': 1673,\n",
              " '975': 43708,\n",
              " '976': 7460,\n",
              " '977': 1059,\n",
              " '978': 45720,\n",
              " '979': 2836,\n",
              " '980': 27788,\n",
              " '981': 2702,\n",
              " '982': 5080,\n",
              " '983': 4816,\n",
              " '984': 7346,\n",
              " '985': 8366,\n",
              " '986': 1235,\n",
              " '987': 3555,\n",
              " '988': 2431,\n",
              " '989': 3910,\n",
              " '990': 1348,\n",
              " '991': 2114,\n",
              " '992': 2138,\n",
              " '993': 63082,\n",
              " '994': 59387,\n",
              " '995': 4380,\n",
              " '996': 2528,\n",
              " '997': 5995,\n",
              " '998': 53123,\n",
              " '999': 27866,\n",
              " ...}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "id2show"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'config' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m EASE\n\u001b[0;32m----> 3\u001b[0m model \u001b[39m=\u001b[39m EASE(\u001b[39m31360\u001b[39;49m, \u001b[39m6807\u001b[39;49m)\n",
            "File \u001b[0;32m/data/ephemeral/home/movie/mission/models.py:302\u001b[0m, in \u001b[0;36mEASE.__init__\u001b[0;34m(self, n_users, n_items)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, n_users, n_items):\n\u001b[1;32m    301\u001b[0m     \u001b[39msuper\u001b[39m(EASE, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m--> 302\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minter_name \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39mINTER_NAME\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    303\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miid_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mitem\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    304\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muid_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
          ]
        }
      ],
      "source": [
        "from models import EASE\n",
        "\n",
        "model = EASE(31360, 6807)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
